#!/usr/bin/env python3
"""
ç®€å•çš„è¯„ä¼°è„šæœ¬
è®¡ç®—F1ã€EMã€å¹³å‡å»¶è¿Ÿç­‰æŒ‡æ ‡ï¼Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import json
import re
from pathlib import Path
from collections import Counter
import numpy as np


def normalize_answer(s):
    """æ ‡å‡†åŒ–ç­”æ¡ˆç”¨äºæ¯”è¾ƒ"""
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)

    def white_space_fix(text):
        return ' '.join(text.split())

    def remove_punc(text):
        exclude = set('!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~')
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))


def compute_f1(prediction, ground_truth):
    """è®¡ç®—F1åˆ†æ•°"""
    pred_tokens = normalize_answer(prediction).split()
    truth_tokens = normalize_answer(ground_truth).split()

    if len(pred_tokens) == 0 or len(truth_tokens) == 0:
        return int(pred_tokens == truth_tokens)

    common_tokens = Counter(pred_tokens) & Counter(truth_tokens)
    num_same = sum(common_tokens.values())

    if num_same == 0:
        return 0

    precision = num_same / len(pred_tokens)
    recall = num_same / len(truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)

    return f1


def compute_exact_match(prediction, ground_truth):
    """è®¡ç®—ç²¾ç¡®åŒ¹é…"""
    return int(normalize_answer(prediction) == normalize_answer(ground_truth))


def evaluate_predictions(predictions):
    """è¯„ä¼°é¢„æµ‹ç»“æœ"""
    f1_scores = []
    em_scores = []
    latencies = []
    successes = []

    for pred in predictions:
        if pred['success']:
            f1 = compute_f1(pred['predicted_answer'], pred['gold_answer'])
            em = compute_exact_match(pred['predicted_answer'], pred['gold_answer'])

            f1_scores.append(f1)
            em_scores.append(em)
            latencies.append(pred['latency'])
            successes.append(True)
        else:
            successes.append(False)

    return {
        'f1_mean': np.mean(f1_scores) if f1_scores else 0,
        'f1_std': np.std(f1_scores) if f1_scores else 0,
        'em_mean': np.mean(em_scores) if em_scores else 0,
        'em_std': np.std(em_scores) if em_scores else 0,
        'latency_mean': np.mean(latencies) if latencies else 0,
        'latency_std': np.std(latencies) if latencies else 0,
        'latency_median': np.median(latencies) if latencies else 0,
        'success_rate': sum(successes) / len(successes) if successes else 0,
        'total': len(predictions),
        'successful': sum(successes)
    }


def generate_comparison_report(baseline_metrics, hipporag_metrics):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    report = []

    report.append("# HippoRAGä¼˜åŒ–ç‰ˆå®éªŒå¯¹æ¯”æŠ¥å‘Š")
    report.append("")
    report.append("## å®éªŒé…ç½®")
    report.append("")
    report.append("| å‚æ•° | å€¼ |")
    report.append("|------|---|")
    report.append("| æ–‡æ¡£æ•° | 66,573 (å®Œæ•´) |")
    report.append("| KGè¦†ç›–ç‡ | 70% |")
    report.append("| éªŒè¯é›†å¤§å° | 500 é—®é¢˜ |")
    report.append("| Embeddingæ¨¡å‹ | text-embedding-3-small |")
    report.append("| LLMæ¨¡å‹ | gpt-3.5-turbo |")
    report.append("")

    report.append("## æ•´ä½“æ€§èƒ½å¯¹æ¯”")
    report.append("")
    report.append("| æ–¹æ³• | F1 Score | Exact Match | å¹³å‡å»¶è¿Ÿ (s) | æˆåŠŸç‡ |")
    report.append("|------|----------|-------------|-------------|--------|")

    baseline_line = f"| Baseline RAG | {baseline_metrics['f1_mean']:.4f} Â± {baseline_metrics['f1_std']:.4f} | "
    baseline_line += f"{baseline_metrics['em_mean']*100:.1f}% Â± {baseline_metrics['em_std']*100:.1f}% | "
    baseline_line += f"{baseline_metrics['latency_mean']:.2f} Â± {baseline_metrics['latency_std']:.2f} | "
    baseline_line += f"{baseline_metrics['success_rate']*100:.1f}% |"
    report.append(baseline_line)

    hipporag_line = f"| HippoRAG | {hipporag_metrics['f1_mean']:.4f} Â± {hipporag_metrics['f1_std']:.4f} | "
    hipporag_line += f"{hipporag_metrics['em_mean']*100:.1f}% Â± {hipporag_metrics['em_std']*100:.1f}% | "
    hipporag_line += f"{hipporag_metrics['latency_mean']:.2f} Â± {hipporag_metrics['latency_std']:.2f} | "
    hipporag_line += f"{hipporag_metrics['success_rate']*100:.1f}% |"
    report.append(hipporag_line)

    report.append("")

    # è®¡ç®—æå‡
    f1_improvement = (hipporag_metrics['f1_mean'] - baseline_metrics['f1_mean']) / baseline_metrics['f1_mean'] * 100
    em_improvement = (hipporag_metrics['em_mean'] - baseline_metrics['em_mean']) / baseline_metrics['em_mean'] * 100
    latency_change = (hipporag_metrics['latency_mean'] - baseline_metrics['latency_mean']) / baseline_metrics['latency_mean'] * 100

    report.append("## æ€§èƒ½æå‡")
    report.append("")
    report.append("| æŒ‡æ ‡ | æå‡ |")
    report.append("|------|------|")
    report.append(f"| F1 Score | {f1_improvement:+.1f}% |")
    report.append(f"| Exact Match | {em_improvement:+.1f}% |")
    report.append(f"| å¹³å‡å»¶è¿Ÿ | {latency_change:+.1f}% |")
    report.append("")

    # è¯¦ç»†ç»Ÿè®¡
    report.append("## è¯¦ç»†ç»Ÿè®¡")
    report.append("")
    report.append("### Baseline RAG")
    report.append("")
    report.append(f"- æ€»é—®é¢˜æ•°: {baseline_metrics['total']}")
    report.append(f"- æˆåŠŸæ•°: {baseline_metrics['successful']}")
    report.append(f"- æˆåŠŸç‡: {baseline_metrics['success_rate']*100:.1f}%")
    report.append(f"- F1: {baseline_metrics['f1_mean']:.4f} Â± {baseline_metrics['f1_std']:.4f}")
    report.append(f"- EM: {baseline_metrics['em_mean']*100:.1f}% Â± {baseline_metrics['em_std']*100:.1f}%")
    report.append(f"- å¹³å‡å»¶è¿Ÿ: {baseline_metrics['latency_mean']:.2f}s")
    report.append(f"- ä¸­ä½å»¶è¿Ÿ: {baseline_metrics['latency_median']:.2f}s")
    report.append("")

    report.append("### HippoRAG")
    report.append("")
    report.append(f"- æ€»é—®é¢˜æ•°: {hipporag_metrics['total']}")
    report.append(f"- æˆåŠŸæ•°: {hipporag_metrics['successful']}")
    report.append(f"- æˆåŠŸç‡: {hipporag_metrics['success_rate']*100:.1f}%")
    report.append(f"- F1: {hipporag_metrics['f1_mean']:.4f} Â± {hipporag_metrics['f1_std']:.4f}")
    report.append(f"- EM: {hipporag_metrics['em_mean']*100:.1f}% Â± {hipporag_metrics['em_std']*100:.1f}%")
    report.append(f"- å¹³å‡å»¶è¿Ÿ: {hipporag_metrics['latency_mean']:.2f}s")
    report.append(f"- ä¸­ä½å»¶è¿Ÿ: {hipporag_metrics['latency_median']:.2f}s")
    report.append("")

    # ç»“è®º
    report.append("## ç»“è®º")
    report.append("")
    if f1_improvement > 10:
        report.append(f"âœ… **HippoRAGæ˜¾è‘—ä¼˜äºBaseline**ï¼ŒF1æå‡{f1_improvement:.1f}%ï¼ŒéªŒè¯äº†çŸ¥è¯†å›¾è°±åœ¨å¤šè·³æ¨ç†ä¸­çš„ä»·å€¼ã€‚")
    elif f1_improvement > 5:
        report.append(f"âœ“ **HippoRAGç•¥ä¼˜äºBaseline**ï¼ŒF1æå‡{f1_improvement:.1f}%ï¼Œåœ¨å¤šè·³æ¨ç†åœºæ™¯æœ‰ä¸€å®šä¼˜åŠ¿ã€‚")
    elif f1_improvement > -5:
        report.append(f"â– **HippoRAGä¸BaselineåŸºæœ¬æŒå¹³**ï¼ŒF1å˜åŒ–{f1_improvement:.1f}%ï¼Œå¯èƒ½åŸå› ï¼š")
        report.append("- éªŒè¯é›†åŒ…å«è¾ƒå¤šå•è·³é—®é¢˜")
        report.append("- å®ä½“æå–ä¸å¤Ÿå‡†ç¡®")
        report.append("- PPRæƒé‡éœ€è¦è°ƒä¼˜")
    else:
        report.append(f"âš ï¸ **HippoRAGå¼±äºBaseline**ï¼ŒF1ä¸‹é™{abs(f1_improvement):.1f}%ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æåŸå› ã€‚")

    report.append("")

    return '\n'.join(report)


def main():
    """ä¸»å‡½æ•°"""
    print('='*70)
    print('è¯„ä¼°å®éªŒç»“æœ')
    print('='*70)

    # åŠ è½½ç»“æœ
    print('\nğŸ“Š åŠ è½½é¢„æµ‹ç»“æœ...')

    baseline_file = 'results/baseline_full/predictions.json'
    hipporag_file = 'results/hipporag_full/predictions.json'

    if not Path(baseline_file).exists():
        print(f'âœ— Baselineç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {baseline_file}')
        return 1

    if not Path(hipporag_file).exists():
        print(f'âœ— HippoRAGç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {hipporag_file}')
        return 1

    with open(baseline_file, 'r') as f:
        baseline_predictions = json.load(f)
    print(f'âœ… Baseline: {len(baseline_predictions)} ä¸ªé¢„æµ‹')

    with open(hipporag_file, 'r') as f:
        hipporag_predictions = json.load(f)
    print(f'âœ… HippoRAG: {len(hipporag_predictions)} ä¸ªé¢„æµ‹')

    # è¯„ä¼°
    print('\nğŸ”¬ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...')

    baseline_metrics = evaluate_predictions(baseline_predictions)
    print(f'âœ… Baseline - F1: {baseline_metrics["f1_mean"]:.4f}, EM: {baseline_metrics["em_mean"]*100:.1f}%')

    hipporag_metrics = evaluate_predictions(hipporag_predictions)
    print(f'âœ… HippoRAG - F1: {hipporag_metrics["f1_mean"]:.4f}, EM: {hipporag_metrics["em_mean"]*100:.1f}%')

    # ç”ŸæˆæŠ¥å‘Š
    print('\nğŸ“ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...')

    report = generate_comparison_report(baseline_metrics, hipporag_metrics)

    # ä¿å­˜æŠ¥å‘Š
    output_file = 'results/comparison_report.md'
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w') as f:
        f.write(report)

    print(f'âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}')

    # ä¿å­˜JSONæŒ‡æ ‡
    metrics_file = 'results/evaluation_metrics.json'
    with open(metrics_file, 'w') as f:
        json.dump({
            'baseline': baseline_metrics,
            'hipporag': hipporag_metrics
        }, f, indent=2)

    print(f'âœ… æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_file}')

    print('\n' + '='*70)
    print('âœ… è¯„ä¼°å®Œæˆï¼')
    print('='*70)

    # æ˜¾ç¤ºç®€è¦ç»“æœ
    print('\n' + report)

    return 0


if __name__ == '__main__':
    import sys
    sys.exit(main())
