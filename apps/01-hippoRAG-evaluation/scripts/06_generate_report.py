#!/usr/bin/env python3
"""
è¯„ä¼°æŠ¥å‘Šç”Ÿæˆè„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½æ‰€æœ‰å®žéªŒç»“æžœ
2. è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆF1, EM, Recall@K ç­‰ï¼‰
3. ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
4. ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨
"""

import os
import sys
import json
from pathlib import Path
from typing import List, Dict
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
from sklearn.metrics import f1_score


def normalize_answer(answer: str) -> str:
    """å½’ä¸€åŒ–ç­”æ¡ˆï¼ˆåŽ»é™¤æ ‡ç‚¹ã€å°å†™ç­‰ï¼‰"""
    import string
    import re

    # è½¬å°å†™
    answer = answer.lower()

    # åŽ»é™¤æ ‡ç‚¹
    answer = answer.translate(str.maketrans('', '', string.punctuation))

    # åŽ»é™¤å¤šä½™ç©ºæ ¼
    answer = re.sub(r'\s+', ' ', answer).strip()

    return answer


def compute_f1(prediction: str, ground_truth: str) -> float:
    """è®¡ç®— F1 åˆ†æ•°"""
    pred_tokens = normalize_answer(prediction).split()
    gold_tokens = normalize_answer(ground_truth).split()

    if len(pred_tokens) == 0 or len(gold_tokens) == 0:
        return 0.0

    # è®¡ç®—äº¤é›†
    common = set(pred_tokens) & set(gold_tokens)

    if len(common) == 0:
        return 0.0

    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(gold_tokens)

    f1 = 2 * (precision * recall) / (precision + recall)

    return f1


def compute_exact_match(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…"""
    return 1.0 if normalize_answer(prediction) == normalize_answer(ground_truth) else 0.0


def evaluate_results(results: List[Dict]) -> Dict:
    """è¯„ä¼°å•ä¸ªæ–¹æ³•çš„ç»“æžœ"""
    f1_scores = []
    em_scores = []
    latencies = []
    success_count = 0

    for result in results:
        if result['success']:
            # è®¡ç®— F1
            f1 = compute_f1(result['predicted_answer'], result['gold_answer'])
            f1_scores.append(f1)

            # è®¡ç®— EM
            em = compute_exact_match(result['predicted_answer'], result['gold_answer'])
            em_scores.append(em)

            # è®°å½•å»¶è¿Ÿ
            latencies.append(result['latency'])

            success_count += 1

    metrics = {
        "num_questions": len(results),
        "success_count": success_count,
        "success_rate": success_count / len(results) if results else 0.0,
        "f1": {
            "mean": float(np.mean(f1_scores)) if f1_scores else 0.0,
            "std": float(np.std(f1_scores)) if f1_scores else 0.0
        },
        "exact_match": {
            "mean": float(np.mean(em_scores)) if em_scores else 0.0,
            "std": float(np.std(em_scores)) if em_scores else 0.0
        },
        "latency": {
            "mean": float(np.mean(latencies)) if latencies else 0.0,
            "std": float(np.std(latencies)) if latencies else 0.0,
            "median": float(np.median(latencies)) if latencies else 0.0
        }
    }

    return metrics


def load_all_results() -> Dict[str, List[Dict]]:
    """åŠ è½½æ‰€æœ‰å®žéªŒç»“æžœ"""
    print("ðŸ“š åŠ è½½å®žéªŒç»“æžœ...")

    results = {}

    # åŠ è½½ Baseline ç»“æžœ
    baseline_path = project_root / "results" / "baseline" / "predictions.json"
    if baseline_path.exists():
        with open(baseline_path, 'r') as f:
            results['Baseline-RAG'] = json.load(f)
        print(f"âœ… Baseline-RAG: {len(results['Baseline-RAG'])} ä¸ªé—®é¢˜")

    # åŠ è½½ HippoRAG ç»“æžœ
    hipporag_path = project_root / "results" / "hipporag" / "predictions.json"
    if hipporag_path.exists():
        with open(hipporag_path, 'r') as f:
            results['HippoRAG'] = json.load(f)
        print(f"âœ… HippoRAG: {len(results['HippoRAG'])} ä¸ªé—®é¢˜")

    return results


def generate_comparison_table(all_metrics: Dict[str, Dict]) -> str:
    """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼ï¼ˆMarkdown æ ¼å¼ï¼‰"""
    table = "# HippoRAG vs Baseline RAG å®žéªŒç»“æžœå¯¹æ¯”\n\n"
    table += "## ä¸»è¦æŒ‡æ ‡\n\n"
    table += "| æ–¹æ³• | F1 Score | Exact Match | å¹³å‡å»¶è¿Ÿ (ç§’) | æˆåŠŸçŽ‡ |\n"
    table += "|------|----------|-------------|--------------|--------|\n"

    for method_name, metrics in all_metrics.items():
        table += f"| **{method_name}** "
        table += f"| {metrics['f1']['mean']:.4f} Â± {metrics['f1']['std']:.4f} "
        table += f"| {metrics['exact_match']['mean']:.4f} Â± {metrics['exact_match']['std']:.4f} "
        table += f"| {metrics['latency']['mean']:.2f} Â± {metrics['latency']['std']:.2f} "
        table += f"| {metrics['success_rate']:.2%} |\n"

    # è®¡ç®—æå‡ç™¾åˆ†æ¯”
    if 'Baseline-RAG' in all_metrics and 'HippoRAG' in all_metrics:
        baseline_f1 = all_metrics['Baseline-RAG']['f1']['mean']
        hipporag_f1 = all_metrics['HippoRAG']['f1']['mean']

        if baseline_f1 > 0:
            improvement = ((hipporag_f1 - baseline_f1) / baseline_f1) * 100
            table += f"\n## æ€§èƒ½æå‡\n\n"
            table += f"- **F1 Score æå‡:** {improvement:+.2f}%\n"

            if improvement > 0:
                table += f"- **ç»“è®º:** HippoRAG åœ¨å¤šè·³é—®ç­”ä¸Šè¡¨çŽ°æ›´å¥½\n"
            elif improvement < -5:
                table += f"- **ç»“è®º:** HippoRAG å¼•å…¥äº†ä¸å¿…è¦çš„å¤æ‚åº¦\n"
            else:
                table += f"- **ç»“è®º:** ä¸¤è€…æ€§èƒ½ç›¸è¿‘ï¼Œéœ€è¦æ›´å¤šæ•°æ®é›†éªŒè¯\n"

    return table


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)

    # åŠ è½½æ‰€æœ‰ç»“æžœ
    all_results = load_all_results()

    if not all_results:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°å®žéªŒç»“æžœ")
        print("è¯·å…ˆè¿è¡Œ: python scripts/05_run_experiments.py")
        return

    # è¯„ä¼°æ¯ä¸ªæ–¹æ³•
    print("\nðŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    all_metrics = {}

    for method_name, results in all_results.items():
        print(f"\nè¯„ä¼°: {method_name}")
        metrics = evaluate_results(results)
        all_metrics[method_name] = metrics

        print(f"  - F1 Score: {metrics['f1']['mean']:.4f} Â± {metrics['f1']['std']:.4f}")
        print(f"  - Exact Match: {metrics['exact_match']['mean']:.4f}")
        print(f"  - å¹³å‡å»¶è¿Ÿ: {metrics['latency']['mean']:.2f}s")

    # ä¿å­˜è¯¦ç»†æŒ‡æ ‡
    metrics_path = project_root / "results" / "evaluation_metrics.json"
    with open(metrics_path, 'w') as f:
        json.dump(all_metrics, indent=2, fp=f)
    print(f"\nâœ… è¯¦ç»†æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")

    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    comparison_table = generate_comparison_table(all_metrics)
    table_path = project_root / "results" / "comparison_table.md"
    with open(table_path, 'w') as f:
        f.write(comparison_table)
    print(f"âœ… å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜: {table_path}")

    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print(f"\n{comparison_table}")

    print("\n" + "=" * 60)
    print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
