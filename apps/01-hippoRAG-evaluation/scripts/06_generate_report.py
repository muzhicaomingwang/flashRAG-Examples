#!/usr/bin/env python3
"""
è¯„ä¼°æŠ¥å‘Šç”Ÿæˆè„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½æ‰€æœ‰å®éªŒç»“æœ
2. è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆF1, EM, Recall@K ç­‰ï¼‰
3. ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
4. ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨
"""

import os
import sys
import json
from datetime import date
from pathlib import Path
from typing import List, Dict
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import yaml
from sklearn.metrics import f1_score


def normalize_answer(answer: str) -> str:
    """å½’ä¸€åŒ–ç­”æ¡ˆï¼ˆå»é™¤æ ‡ç‚¹ã€å°å†™ç­‰ï¼‰"""
    import string
    import re

    # è½¬å°å†™
    answer = answer.lower()

    # å»é™¤æ ‡ç‚¹
    answer = answer.translate(str.maketrans('', '', string.punctuation))

    # å»é™¤å¤šä½™ç©ºæ ¼
    answer = re.sub(r'\s+', ' ', answer).strip()

    return answer


def compute_f1(prediction: str, ground_truth: str) -> float:
    """è®¡ç®— F1 åˆ†æ•°"""
    pred_tokens = normalize_answer(prediction).split()
    gold_tokens = normalize_answer(ground_truth).split()

    if len(pred_tokens) == 0 or len(gold_tokens) == 0:
        return 0.0

    # è®¡ç®—äº¤é›†
    common = set(pred_tokens) & set(gold_tokens)

    if len(common) == 0:
        return 0.0

    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(gold_tokens)

    f1 = 2 * (precision * recall) / (precision + recall)

    return f1


def compute_exact_match(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…"""
    return 1.0 if normalize_answer(prediction) == normalize_answer(ground_truth) else 0.0


def evaluate_results(results: List[Dict]) -> Dict:
    """è¯„ä¼°å•ä¸ªæ–¹æ³•çš„ç»“æœ"""
    f1_scores = []
    em_scores = []
    latencies = []
    success_count = 0

    for result in results:
        if result['success']:
            # è®¡ç®— F1
            f1 = compute_f1(result['predicted_answer'], result['gold_answer'])
            f1_scores.append(f1)

            # è®¡ç®— EM
            em = compute_exact_match(result['predicted_answer'], result['gold_answer'])
            em_scores.append(em)

            # è®°å½•å»¶è¿Ÿ
            latencies.append(result['latency'])

            success_count += 1

    metrics = {
        "num_questions": len(results),
        "success_count": success_count,
        "success_rate": success_count / len(results) if results else 0.0,
        "f1": {
            "mean": float(np.mean(f1_scores)) if f1_scores else 0.0,
            "std": float(np.std(f1_scores)) if f1_scores else 0.0
        },
        "exact_match": {
            "mean": float(np.mean(em_scores)) if em_scores else 0.0,
            "std": float(np.std(em_scores)) if em_scores else 0.0
        },
        "latency": {
            "mean": float(np.mean(latencies)) if latencies else 0.0,
            "std": float(np.std(latencies)) if latencies else 0.0,
            "median": float(np.median(latencies)) if latencies else 0.0
        }
    }

    return metrics


def load_all_results() -> Dict[str, List[Dict]]:
    """åŠ è½½æ‰€æœ‰å®éªŒç»“æœ"""
    print("ğŸ“š åŠ è½½å®éªŒç»“æœ...")

    results = {}

    # åŠ è½½ Baseline ç»“æœ
    baseline_path = project_root / "results" / "baseline" / "predictions.json"
    if baseline_path.exists():
        with open(baseline_path, 'r') as f:
            results['Baseline-RAG'] = json.load(f)
        print(f"âœ… Baseline-RAG: {len(results['Baseline-RAG'])} ä¸ªé—®é¢˜")

    # åŠ è½½ HippoRAG ç»“æœ
    hipporag_path = project_root / "results" / "hipporag" / "predictions.json"
    if hipporag_path.exists():
        with open(hipporag_path, 'r') as f:
            results['HippoRAG'] = json.load(f)
        print(f"âœ… HippoRAG: {len(results['HippoRAG'])} ä¸ªé—®é¢˜")

    return results


def generate_comparison_table(all_metrics: Dict[str, Dict]) -> str:
    """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼ï¼ˆMarkdown æ ¼å¼ï¼‰"""
    table = "# HippoRAG vs Baseline RAG å®éªŒç»“æœå¯¹æ¯”\n\n"
    table += "## ä¸»è¦æŒ‡æ ‡\n\n"
    table += "| æ–¹æ³• | F1 Score | Exact Match | å¹³å‡å»¶è¿Ÿ (ç§’) | æˆåŠŸç‡ |\n"
    table += "|------|----------|-------------|--------------|--------|\n"

    for method_name, metrics in all_metrics.items():
        table += f"| **{method_name}** "
        table += f"| {metrics['f1']['mean']:.4f} Â± {metrics['f1']['std']:.4f} "
        table += f"| {metrics['exact_match']['mean']:.4f} Â± {metrics['exact_match']['std']:.4f} "
        table += f"| {metrics['latency']['mean']:.2f} Â± {metrics['latency']['std']:.2f} "
        table += f"| {metrics['success_rate']:.2%} |\n"

    # è®¡ç®—æå‡ç™¾åˆ†æ¯”
    if 'Baseline-RAG' in all_metrics and 'HippoRAG' in all_metrics:
        baseline_f1 = all_metrics['Baseline-RAG']['f1']['mean']
        hipporag_f1 = all_metrics['HippoRAG']['f1']['mean']

        if baseline_f1 > 0:
            improvement = ((hipporag_f1 - baseline_f1) / baseline_f1) * 100
            table += f"\n## æ€§èƒ½æå‡\n\n"
            table += f"- **F1 Score æå‡:** {improvement:+.2f}%\n"

            if improvement > 0:
                table += f"- **ç»“è®º:** HippoRAG åœ¨å¤šè·³é—®ç­”ä¸Šè¡¨ç°æ›´å¥½\n"
            elif improvement < -5:
                table += f"- **ç»“è®º:** HippoRAG å¼•å…¥äº†ä¸å¿…è¦çš„å¤æ‚åº¦\n"
            else:
                table += f"- **ç»“è®º:** ä¸¤è€…æ€§èƒ½ç›¸è¿‘ï¼Œéœ€è¦æ›´å¤šæ•°æ®é›†éªŒè¯\n"

    return table


def load_config() -> Dict:
    """åŠ è½½å®éªŒé…ç½®"""
    config_path = project_root / "configs" / "experiment_config.yaml"
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def generate_experiment_record(config: Dict, all_metrics: Dict[str, Dict]) -> str:
    """ç”Ÿæˆå¯å¤ç°å®éªŒè®°å½•"""
    dataset = config.get("dataset", {})
    doc_proc = config.get("document_processing", {})
    embedding = config.get("embedding", {})
    faiss_cfg = config.get("faiss", {})
    baseline = config.get("baseline_rag", {})
    hipporag = config.get("hipporag", {})
    retrieval = hipporag.get("retrieval", {})
    pagerank = hipporag.get("pagerank", {})

    lines = []
    lines.append("# Experiment Record")
    lines.append("")
    lines.append("## Run Metadata")
    lines.append(f"- Date (YYYY-MM-DD): {date.today().isoformat()}")
    lines.append("- Operator:")
    lines.append("- Purpose / Hypothesis:")
    lines.append(f"- Dataset: {dataset.get('name', '')}")
    lines.append(f"- Split: {dataset.get('split', '')}")
    lines.append(f"- Sample size (max_samples): {dataset.get('max_samples', '')}")
    lines.append(f"- Random seed: {dataset.get('random_seed', '')}")
    lines.append("")
    lines.append("## Code & Environment")
    lines.append(f"- Repo path: {project_root}")
    lines.append("- Git branch:")
    lines.append("- Git commit:")
    lines.append("- Dirty worktree (yes/no):")
    lines.append("- Python version:")
    lines.append("- OS:")
    lines.append("")
    lines.append("## Model & API")
    lines.append(f"- LLM (generation): baseline={baseline.get('llm_model', '')}; hipporag={retrieval.get('llm_model', '')}")
    lines.append(f"- Embedding model: {embedding.get('model', '')}")
    lines.append(f"- Temperature: {baseline.get('llm_temperature', '')}")
    lines.append(f"- Max tokens: {baseline.get('llm_max_tokens', '')}")
    lines.append("- API provider:")
    lines.append("- Rate limit / concurrency:")
    lines.append("")
    lines.append("## Retrieval & Index")
    lines.append(f"- Baseline top_k: {baseline.get('top_k', '')}")
    lines.append(f"- HippoRAG initial_k: {retrieval.get('initial_k', '')}")
    lines.append(f"- HippoRAG rerank_k: {retrieval.get('rerank_k', '')}")
    lines.append(f"- FAISS index type: {faiss_cfg.get('index_type', '')}")
    lines.append(f"- Normalize vectors: {str(faiss_cfg.get('normalize_vectors', '')).lower()}")
    lines.append("")
    lines.append("## Document Processing")
    lines.append(f"- Chunk size: {doc_proc.get('chunk_size', '')}")
    lines.append(f"- Chunk overlap: {doc_proc.get('chunk_overlap', '')}")
    lines.append(f"- Separators: {doc_proc.get('separators', '')}")
    lines.append(f"- Tokenizer: {doc_proc.get('tokenizer', '')}")
    lines.append("")
    lines.append("## HippoRAG Graph")
    lines.append(f"- Sampling ratio: {hipporag.get('sampling_ratio', '')}")
    lines.append(f"- NER method: {hipporag.get('entity_extraction', {}).get('method', '')}")
    lines.append(f"- RE method: {hipporag.get('relation_extraction', {}).get('method', '')}")
    lines.append(f"- PageRank damping factor: {pagerank.get('damping_factor', '')}")
    lines.append(f"- PageRank max iterations: {pagerank.get('max_iterations', '')}")
    lines.append("")
    lines.append("## Runs")
    lines.append("- Baseline RAG: results/baseline/predictions.json")
    lines.append("- HippoRAG: results/hipporag/predictions.json")
    lines.append("")
    lines.append("## Metrics")
    baseline_metrics = all_metrics.get("Baseline-RAG", {})
    hipporag_metrics = all_metrics.get("HippoRAG", {})
    if baseline_metrics:
        lines.append(f"- Baseline F1: {baseline_metrics['f1']['mean']:.4f}")
        lines.append(f"- Baseline EM: {baseline_metrics['exact_match']['mean']:.4f}")
    if hipporag_metrics:
        lines.append(f"- HippoRAG F1: {hipporag_metrics['f1']['mean']:.4f}")
        lines.append(f"- HippoRAG EM: {hipporag_metrics['exact_match']['mean']:.4f}")
    lines.append("")
    lines.append("## Notes / Anomalies")
    lines.append("-")
    lines.append("")
    lines.append("## Comparison")
    lines.append("- Against prior run (commit/date):")
    lines.append("- Key deltas:")
    lines.append("")

    return "\n".join(lines)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)

    # åŠ è½½æ‰€æœ‰ç»“æœ
    all_results = load_all_results()

    if not all_results:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°å®éªŒç»“æœ")
        print("è¯·å…ˆè¿è¡Œ: python scripts/05_run_experiments.py")
        return

    # è¯„ä¼°æ¯ä¸ªæ–¹æ³•
    print("\nğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    all_metrics = {}

    for method_name, results in all_results.items():
        print(f"\nè¯„ä¼°: {method_name}")
        metrics = evaluate_results(results)
        all_metrics[method_name] = metrics

        print(f"  - F1 Score: {metrics['f1']['mean']:.4f} Â± {metrics['f1']['std']:.4f}")
        print(f"  - Exact Match: {metrics['exact_match']['mean']:.4f}")
        print(f"  - å¹³å‡å»¶è¿Ÿ: {metrics['latency']['mean']:.2f}s")

    # ä¿å­˜è¯¦ç»†æŒ‡æ ‡
    metrics_path = project_root / "results" / "evaluation_metrics.json"
    with open(metrics_path, 'w') as f:
        json.dump(all_metrics, indent=2, fp=f)
    print(f"\nâœ… è¯¦ç»†æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")

    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    comparison_table = generate_comparison_table(all_metrics)
    table_path = project_root / "results" / "comparison_table.md"
    with open(table_path, 'w') as f:
        f.write(comparison_table)
    print(f"âœ… å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜: {table_path}")

    # ç”Ÿæˆå®éªŒè®°å½•ï¼ˆè‡ªåŠ¨å¡«å……åŸºç¡€å­—æ®µï¼‰
    config = load_config()
    record_content = generate_experiment_record(config, all_metrics)
    record_path = project_root / "results" / "experiment_record_latest.md"
    with open(record_path, "w") as f:
        f.write(record_content)
    print(f"âœ… å®éªŒè®°å½•å·²ä¿å­˜: {record_path}")

    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print(f"\n{comparison_table}")

    print("\n" + "=" * 60)
    print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
