#!/usr/bin/env python3
"""
HippoRAG çŸ¥è¯†å›¾è°±æ„å»ºè„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

æ”¹è¿›ï¼š
1. 70%è¦†ç›–ç‡ï¼ˆè€Œé30%ï¼‰
2. æ·»åŠ å®ä½“é—´å…³ç³»ï¼ˆSpaCyä¾å­˜å¥æ³•ï¼‰
3. ä¼˜åŒ–å¤„ç†é€Ÿåº¦
"""

import json
import pickle
import random
from pathlib import Path
import spacy
import networkx as nx

print('='*60)
print('HippoRAG çŸ¥è¯†å›¾è°±æ„å»ºï¼ˆä¼˜åŒ–ç‰ˆï¼‰')
print('='*60)

# 1. åŠ è½½æ–‡æ¡£å—
print('\nğŸ“š æ­¥éª¤1/5: åŠ è½½æ–‡æ¡£å—')
chunks = []
chunks_path = Path('data/processed/hotpotqa_full_chunks.jsonl')

if not chunks_path.exists():
    print('âš ï¸  å®Œæ•´chunksæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨10Kç‰ˆæœ¬')
    chunks_path = Path('data/processed/hotpotqa_chunks.jsonl')

with open(chunks_path, 'r') as f:
    for line in f:
        chunks.append(json.loads(line))

print(f'âœ… åŠ è½½ {len(chunks)} ä¸ªå—')

# 2. é‡‡æ ·70%ï¼ˆä¼˜åŒ–ï¼‰
print('\nğŸ² æ­¥éª¤2/5: é‡‡æ ·70%ï¼ˆä¼˜åŒ–from30%ï¼‰')
random.seed(42)
sampling_ratio = 0.7
n_samples = int(len(chunks) * sampling_ratio)
sampled_chunks = random.sample(chunks, n_samples)
print(f'âœ… é‡‡æ ·: {len(chunks)} â†’ {len(sampled_chunks)} å—ï¼ˆ{sampling_ratio*100:.0f}%ï¼‰')

# 3. åŠ è½½SpaCy
print('\nğŸ”§ æ­¥éª¤3/5: åŠ è½½SpaCyæ¨¡å‹')
nlp = spacy.load('en_core_web_sm')
print('âœ… SpaCyå°±ç»ª')

# 4. æå–å®ä½“å’Œå…³ç³»
print('\nğŸ”¨ æ­¥éª¤4/5: æå–å®ä½“å’Œå…³ç³»')
kg = nx.DiGraph()
entity_counts = {}
relation_counts = {}

for i, chunk in enumerate(sampled_chunks):
    if i % 1000 == 0:
        print(f'   è¿›åº¦: {i}/{len(sampled_chunks)}')

    chunk_id = chunk['chunk_id']
    text = chunk['text']

    # æ·»åŠ chunkèŠ‚ç‚¹
    kg.add_node(chunk_id, type='chunk', text=text[:500])  # åªä¿å­˜å‰500å­—ç¬¦èŠ‚çœå†…å­˜

    # æå–å®ä½“
    doc = nlp(text)
    entities = [(ent.text.lower().strip(), ent.label_) for ent in doc.ents]
    unique_entities = list(set(entities))[:10]

    # æ·»åŠ å®ä½“èŠ‚ç‚¹
    for entity_name, entity_type in unique_entities:
        entity_id = f"entity_{entity_name.replace(' ', '_')}"

        if not kg.has_node(entity_id):
            kg.add_node(entity_id, type='entity', name=entity_name, entity_type=entity_type)

        kg.add_edge(chunk_id, entity_id, relation='contains')
        entity_counts[entity_name] = entity_counts.get(entity_name, 0) + 1

    # æå–å®ä½“é—´å…³ç³»ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨ä¾å­˜å¥æ³•ï¼‰
    for token in doc:
        if token.dep_ in ['nsubj', 'nsubjpass'] and token.head.pos_ == 'VERB':
            # ä¸»è¯­
            subj = token.text.lower()
            verb = token.head.lemma_

            # æŸ¥æ‰¾å®¾è¯­
            for child in token.head.children:
                if child.dep_ in ['dobj', 'attr', 'pobj']:
                    obj = child.text.lower()

                    subj_id = f"entity_{subj.replace(' ', '_')}"
                    obj_id = f"entity_{obj.replace(' ', '_')}"

                    # åªè¿æ¥å›¾ä¸­å·²å­˜åœ¨çš„å®ä½“
                    if kg.has_node(subj_id) and kg.has_node(obj_id):
                        kg.add_edge(subj_id, obj_id, relation=verb)
                        relation_counts[verb] = relation_counts.get(verb, 0) + 1

num_chunks = len([n for n in kg.nodes if kg.nodes[n]['type'] == 'chunk'])
num_entities = len([n for n in kg.nodes if kg.nodes[n]['type'] == 'entity'])

print(f'\nâœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ')
print(f'   - èŠ‚ç‚¹æ€»æ•°: {kg.number_of_nodes():,}')
print(f'   - è¾¹æ€»æ•°: {kg.number_of_edges():,}')
print(f'   - ChunkèŠ‚ç‚¹: {num_chunks:,}')
print(f'   - å®ä½“èŠ‚ç‚¹: {num_entities:,}')
print(f'   - å®ä½“é—´å…³ç³»: {sum(relation_counts.values()):,}')

# 5. PageRank
print('\nğŸ”¨ æ­¥éª¤5/5: è®¡ç®—PageRank')
pagerank_scores = nx.pagerank(kg, alpha=0.85, max_iter=100)
print(f'âœ… PageRankè®¡ç®—å®Œæˆ')

# Topå®ä½“
print(f'\nğŸ“Š Top-10 æœ€é‡è¦å®ä½“:')
entity_scores = {n: s for n, s in pagerank_scores.items() if kg.nodes[n]['type'] == 'entity'}
for node_id, score in sorted(entity_scores.items(), key=lambda x: x[1], reverse=True)[:10]:
    entity_name = kg.nodes[node_id]['name']
    print(f'   - {entity_name}: {score:.6f}')

# ä¿å­˜
print('\nğŸ’¾ ä¿å­˜çŸ¥è¯†å›¾è°±...')
Path('data/knowledge_graphs').mkdir(parents=True, exist_ok=True)

with open('data/knowledge_graphs/hotpotqa_kg_full.gpickle', 'wb') as f:
    pickle.dump(kg, f)
print(f'âœ… å›¾è°±å·²ä¿å­˜: hotpotqa_kg_full.gpickle')

with open('data/knowledge_graphs/hotpotqa_pagerank_full.pkl', 'wb') as f:
    pickle.dump(pagerank_scores, f)
print(f'âœ… PageRankå·²ä¿å­˜')

# ç»Ÿè®¡
stats = {
    'num_nodes': kg.number_of_nodes(),
    'num_edges': kg.number_of_edges(),
    'num_chunks': num_chunks,
    'num_entities': num_entities,
    'num_entity_relations': sum(relation_counts.values()),
    'sampling_ratio': sampling_ratio,
    'top_entities': dict(sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:20]),
    'top_relations': dict(sorted(relation_counts.items(), key=lambda x: x[1], reverse=True)[:20])
}

with open('data/knowledge_graphs/hotpotqa_kg_full_stats.json', 'w') as f:
    json.dump(stats, indent=2, fp=f)
print(f'âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜')

print('\n' + '='*60)
print('ğŸ‰ ä¼˜åŒ–ç‰ˆHippoRAGçŸ¥è¯†å›¾è°±æ„å»ºæˆåŠŸï¼')
print('='*60)
print(f'\næ”¹è¿›ç‚¹ï¼š')
print(f'  - è¦†ç›–ç‡: 30% â†’ 70%')
print(f'  - æ·»åŠ å®ä½“é—´å…³ç³»: {sum(relation_counts.values()):,}æ¡')
print(f'  - å›¾æ›´å¯†é›†ï¼Œè¿æ¥æ›´ä¸°å¯Œ')
