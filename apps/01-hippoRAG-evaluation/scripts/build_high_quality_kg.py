#!/usr/bin/env python3
"""
æ„å»ºé«˜è´¨é‡çŸ¥è¯†å›¾è°±
ä½¿ç”¨LLMæå–å®ä½“é—´çš„å…³ç³»ï¼Œå¢åŠ å›¾å¯†åº¦
"""

import json
import pickle
import os
from pathlib import Path
import networkx as nx
from openai import OpenAI
import spacy
from tqdm import tqdm
import time

print('='*80)
print('æ„å»ºé«˜è´¨é‡çŸ¥è¯†å›¾è°±ï¼ˆå¢åŠ å®ä½“é—´å…³ç³»ï¼‰')
print('='*80)

# åˆå§‹åŒ–
client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))
nlp = spacy.load('en_core_web_sm')

# åŠ è½½chunks
print('\nğŸ“š åŠ è½½æ•°æ®...')
with open('data/indices/faiss/hotpotqa_full_docs.pkl', 'rb') as f:
    chunks = pickle.load(f)

# é‡‡æ ·70%ç”¨äºKGæ„å»ºï¼ˆä¸ä¹‹å‰ä¿æŒä¸€è‡´ï¼‰
import random
random.seed(42)
sampled_chunks = random.sample(chunks, int(len(chunks) * 0.7))
print(f'âœ… åŠ è½½äº† {len(chunks):,} chunks')
print(f'âœ… é‡‡æ · {len(sampled_chunks):,} chunks (70%) ç”¨äºKGæ„å»º')

# åˆ›å»ºæ–°çš„KG
kg = nx.Graph()
print('\nğŸ”¨ æ„å»ºçŸ¥è¯†å›¾è°±...')

# ç¬¬ä¸€é˜¶æ®µï¼šæ·»åŠ chunkå’Œå®ä½“èŠ‚ç‚¹
print('\nç¬¬ä¸€é˜¶æ®µï¼šæå–å®ä½“å¹¶å»ºç«‹Chunkâ†’Entityè¿æ¥')
chunk_entities_map = {}

for i, chunk in enumerate(tqdm(sampled_chunks, desc='æå–å®ä½“')):
    chunk_id = chunk['chunk_id']
    text = chunk['text']

    # æ·»åŠ chunkèŠ‚ç‚¹
    kg.add_node(chunk_id, type='chunk', text=text[:200])

    # æå–å®ä½“
    doc = nlp(text)
    entities = []

    for ent in doc.ents:
        entity_text = ent.text.lower().strip()
        entity_id = f"entity_{entity_text.replace(' ', '_')}"
        entities.append(entity_id)

        # æ·»åŠ å®ä½“èŠ‚ç‚¹
        if not kg.has_node(entity_id):
            kg.add_node(entity_id, type='entity', name=entity_text, label=ent.label_)

        # æ·»åŠ chunkâ†’entityè¾¹
        kg.add_edge(chunk_id, entity_id, relation='contains')

    chunk_entities_map[chunk_id] = entities

print(f'\nâœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆ:')
print(f'   èŠ‚ç‚¹æ•°: {kg.number_of_nodes():,}')
print(f'   è¾¹æ•°: {kg.number_of_edges():,}')

# ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨LLMæå–å®ä½“é—´å…³ç³»
print('\nç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨LLMæå–å®ä½“é—´å…³ç³»')
print('è¿™å°†è°ƒç”¨OpenAI APIï¼Œé¢„è®¡æˆæœ¬: $2-3, æ—¶é—´: 30-40åˆ†é’Ÿ')
print()

# åªå¯¹åŒ…å«2ä¸ªä»¥ä¸Šå®ä½“çš„chunkæå–å…³ç³»
chunks_with_multiple_entities = [
    (cid, ents) for cid, ents in chunk_entities_map.items()
    if len(ents) >= 2
]

print(f'æœ‰ {len(chunks_with_multiple_entities):,} ä¸ªchunksåŒ…å«2+å®ä½“')
print(f'å°†ä¸ºè¿™äº›chunksæå–å®ä½“é—´å…³ç³»...')
print()

# é‡‡æ ·ç­–ç•¥ï¼šéšæœºé€‰æ‹©5000ä¸ªchunksè¿›è¡Œå…³ç³»æå–ï¼ˆå¹³è¡¡æˆæœ¬å’Œè´¨é‡ï¼‰
MAX_CHUNKS_FOR_RELATION = 5000
if len(chunks_with_multiple_entities) > MAX_CHUNKS_FOR_RELATION:
    chunks_to_process = random.sample(chunks_with_multiple_entities, MAX_CHUNKS_FOR_RELATION)
    print(f'âš ï¸  ä¸ºæ§åˆ¶æˆæœ¬ï¼Œé‡‡æ · {MAX_CHUNKS_FOR_RELATION:,} ä¸ªchunksè¿›è¡Œå…³ç³»æå–')
else:
    chunks_to_process = chunks_with_multiple_entities

total_relations_added = 0
api_calls = 0
total_cost = 0

# æ‰¹å¤„ç†ï¼šæ¯æ¬¡å¤„ç†ä¸€ä¸ªchunk
for i, (chunk_id, entities) in enumerate(tqdm(chunks_to_process, desc='æå–å…³ç³»')):
    if len(entities) < 2:
        continue

    # è·å–chunkæ–‡æœ¬
    chunk_text = next(c['text'] for c in sampled_chunks if c['chunk_id'] == chunk_id)

    # æå–å®ä½“åç§°ï¼ˆå»æ‰entity_å‰ç¼€ï¼‰
    entity_names = [e.replace('entity_', '').replace('_', ' ') for e in entities]

    # æ„å»ºprompt
    prompt = f"""Extract relationships between entities in this text. Only include direct, explicit relationships.

Text: {chunk_text[:500]}

Entities: {', '.join(entity_names[:10])}

Output format (JSON):
{{"relations": [
  {{"subject": "entity1", "relation": "verb/relationship", "object": "entity2"}},
  ...
]}}

Rules:
- Only include relationships explicitly stated in the text
- Use simple verbs (e.g., "founded", "located_in", "part_of", "won")
- Maximum 5 relations per text
- Return empty list if no clear relationships"""

    try:
        response = client.chat.completions.create(
            model='gpt-3.5-turbo',
            messages=[{'role': 'user', 'content': prompt}],
            temperature=0.0,
            max_tokens=300
        )

        api_calls += 1
        cost = response.usage.total_tokens * 0.000002
        total_cost += cost

        # è§£æç»“æœ
        result_text = response.choices[0].message.content

        try:
            # å°è¯•è§£æJSON
            result = json.loads(result_text)
            relations = result.get('relations', [])

            # æ·»åŠ å…³ç³»åˆ°KG
            for rel in relations[:5]:  # æœ€å¤š5ä¸ªå…³ç³»
                subj = rel.get('subject', '').lower().strip().replace(' ', '_')
                relation = rel.get('relation', '').lower().strip()
                obj = rel.get('object', '').lower().strip().replace(' ', '_')

                if subj and obj and relation:
                    subj_id = f"entity_{subj}"
                    obj_id = f"entity_{obj}"

                    # åªæ·»åŠ KGä¸­å·²å­˜åœ¨çš„å®ä½“é—´çš„å…³ç³»
                    if kg.has_node(subj_id) and kg.has_node(obj_id):
                        kg.add_edge(subj_id, obj_id, relation=relation, source='llm')
                        total_relations_added += 1

        except json.JSONDecodeError:
            # LLMè¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡
            pass

        # æ¯100æ¬¡æ˜¾ç¤ºè¿›åº¦
        if (i + 1) % 100 == 0:
            print(f'\n   å¤„ç†: {i+1}/{len(chunks_to_process)}')
            print(f'   APIè°ƒç”¨: {api_calls}, æˆæœ¬: ${total_cost:.2f}')
            print(f'   å·²æ·»åŠ å…³ç³»: {total_relations_added}')

        # é™æµï¼ˆé¿å…è¶…è¿‡APIé€Ÿç‡é™åˆ¶ï¼‰
        if (i + 1) % 50 == 0:
            time.sleep(1)

    except Exception as e:
        print(f'\n   âš ï¸  é”™è¯¯: {str(e)[:100]}')
        continue

print(f'\nâœ… ç¬¬äºŒé˜¶æ®µå®Œæˆ:')
print(f'   APIè°ƒç”¨: {api_calls}')
print(f'   æ€»æˆæœ¬: ${total_cost:.2f}')
print(f'   æ·»åŠ çš„å®ä½“é—´å…³ç³»: {total_relations_added:,}')

# ç»Ÿè®¡æœ€ç»ˆKG
chunk_nodes = [n for n in kg.nodes if not n.startswith('entity_')]
entity_nodes = [n for n in kg.nodes if n.startswith('entity_')]

chunk_to_entity_edges = 0
entity_to_entity_edges = 0

for u, v in kg.edges():
    if (not u.startswith('entity_')) and v.startswith('entity_'):
        chunk_to_entity_edges += 1
    elif u.startswith('entity_') and v.startswith('entity_'):
        entity_to_entity_edges += 1

print(f'\nğŸ“Š æœ€ç»ˆçŸ¥è¯†å›¾è°±ç»Ÿè®¡:')
print(f'   æ€»èŠ‚ç‚¹: {kg.number_of_nodes():,}')
print(f'   - ChunkèŠ‚ç‚¹: {len(chunk_nodes):,}')
print(f'   - å®ä½“èŠ‚ç‚¹: {len(entity_nodes):,}')
print(f'   æ€»è¾¹: {kg.number_of_edges():,}')
print(f'   - Chunkâ†’Entity: {chunk_to_entity_edges:,} ({chunk_to_entity_edges/kg.number_of_edges()*100:.1f}%)')
print(f'   - Entityâ†’Entity: {entity_to_entity_edges:,} ({entity_to_entity_edges/kg.number_of_edges()*100:.1f}%)')

density = kg.number_of_edges() / (kg.number_of_nodes() * (kg.number_of_nodes() - 1)) if kg.number_of_nodes() > 1 else 0
print(f'   å›¾å¯†åº¦: {density:.6f}')

# å¯¹æ¯”æ”¹è¿›
print(f'\nğŸ“ˆ ä¸ä¹‹å‰KGå¯¹æ¯”:')
print(f'   Entityâ†’Entityè¾¹: 6,956 (1.7%) â†’ {entity_to_entity_edges:,} ({entity_to_entity_edges/kg.number_of_edges()*100:.1f}%)')
print(f'   æå‡: {entity_to_entity_edges/6956:.1f}x')

# ä¿å­˜KG
output_dir = Path('data/knowledge_graphs')
output_dir.mkdir(parents=True, exist_ok=True)

with open(output_dir / 'hotpotqa_kg_high_quality.gpickle', 'wb') as f:
    pickle.dump(kg, f, protocol=pickle.HIGHEST_PROTOCOL)

# è®¡ç®—å¹¶ä¿å­˜PageRank
print('\nğŸ”„ è®¡ç®—PageRank...')
pagerank_scores = nx.pagerank(kg, alpha=0.85, max_iter=100)

with open(output_dir / 'hotpotqa_pagerank_high_quality.pkl', 'wb') as f:
    pickle.dump(pagerank_scores, f, protocol=pickle.HIGHEST_PROTOCOL)

# ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
stats = {
    'num_nodes': kg.number_of_nodes(),
    'num_edges': kg.number_of_edges(),
    'num_chunks': len(chunk_nodes),
    'num_entities': len(entity_nodes),
    'num_entity_relations': entity_to_entity_edges,
    'chunk_entity_edges': chunk_to_entity_edges,
    'graph_density': density,
    'sampling_ratio': 0.7,
    'llm_extraction': True,
    'api_calls': api_calls,
    'total_cost': total_cost,
}

with open(output_dir / 'hotpotqa_kg_high_quality_stats.json', 'w') as f:
    json.dump(stats, f, indent=2)

print(f'\nâœ… é«˜è´¨é‡KGæ„å»ºå®Œæˆï¼')
print(f'   ä¿å­˜ä½ç½®: {output_dir}/')
print(f'   - hotpotqa_kg_high_quality.gpickle')
print(f'   - hotpotqa_pagerank_high_quality.pkl')
print(f'   - hotpotqa_kg_high_quality_stats.json')
print(f'\nğŸ’° æ€»æˆæœ¬: ${total_cost:.2f}')
print()
print('ä¸‹ä¸€æ­¥: ä½¿ç”¨é«˜è´¨é‡KGé‡æ–°è¿è¡ŒHippoRAGå®éªŒ')
print('è¿è¡Œ: python scripts/run_experiment_high_quality_kg.py')
