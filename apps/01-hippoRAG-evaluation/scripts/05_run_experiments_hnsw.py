#!/usr/bin/env python3
"""
HNSW å¯¹æ¯”å®éªŒè¿è¡Œè„šæœ¬

åŠŸèƒ½ï¼š
1. è¿è¡Œ Baseline RAGï¼ˆHNSW æ£€ç´¢ï¼‰
2. è¿è¡Œ HippoRAGï¼ˆHNSW åˆæ£€ç´¢ + KG/PPR é‡æ’ï¼‰
3. ä¿å­˜ç»“æœåˆ°ç‹¬ç«‹ç›®å½•
"""

import os
import sys
import json
import pickle
import time
from pathlib import Path
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml
import numpy as np
import faiss
import spacy
import networkx as nx
from tqdm import tqdm
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def load_config() -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = project_root / "configs" / "experiment_config.yaml"
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def load_validation_set() -> List[Dict]:
    """åŠ è½½éªŒè¯é›†"""
    val_path = project_root / "data" / "raw" / "hotpotqa_validation.jsonl"

    validation = []
    with open(val_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line)
            # å…¼å®¹ FlashRAG dev æ ¼å¼ï¼šgolden_answers -> answer
            if 'answer' not in item and 'golden_answers' in item:
                ga = item.get('golden_answers') or []
                item['answer'] = ga[0] if ga else ""
            validation.append(item)

    return validation


def load_hnsw_indices(config: Dict):
    """åŠ è½½ HNSW ç´¢å¼•"""
    # åŠ è½½ FAISS HNSW ç´¢å¼•
    faiss_index_path = project_root / config['faiss_hnsw']['persist_path']
    faiss_index = faiss.read_index(str(faiss_index_path))

    # è®¾ç½® HNSW æœç´¢å‚æ•°
    faiss_index.hnsw.efSearch = int(config['faiss_hnsw']['ef_search'])

    # åŠ è½½æ–‡æ¡£æ˜ å°„
    doc_mapping_path = project_root / config['faiss_hnsw']['doc_mapping_path']
    with open(doc_mapping_path, 'rb') as f:
        chunks = pickle.load(f)

    # å…¼å®¹æ—§æ˜ å°„ï¼šè¡¥é½ title/doc_id/chunk_index
    for chunk in chunks:
        if 'chunk_id' in chunk:
            doc_id, _, idx_str = chunk['chunk_id'].rpartition('_')
            if not doc_id:
                doc_id = chunk['chunk_id']
                idx_str = ""
            if 'doc_id' not in chunk:
                chunk['doc_id'] = doc_id
            if 'title' not in chunk:
                chunk['title'] = doc_id.replace('_', ' ')
            if 'chunk_index' not in chunk and idx_str.isdigit():
                chunk['chunk_index'] = int(idx_str)

    return faiss_index, chunks


def load_hipporag_kg():
    """åŠ è½½ HippoRAG çŸ¥è¯†å›¾è°±"""
    kg_path = project_root / "data" / "knowledge_graphs" / "hotpotqa_kg.gpickle"
    if hasattr(nx, "read_gpickle"):
        kg = nx.read_gpickle(str(kg_path))
    else:
        with open(kg_path, 'rb') as f:
            kg = pickle.load(f)

    pr_path = project_root / "data" / "knowledge_graphs" / "hotpotqa_pagerank.pkl"
    with open(pr_path, 'rb') as f:
        pagerank_scores = pickle.load(f)

    return kg, pagerank_scores


def extract_entities_spacy(text: str, nlp) -> List[Tuple[str, str]]:
    """ä½¿ç”¨ SpaCy æå–å®ä½“"""
    doc = nlp(text)

    entities = []
    for ent in doc.ents:
        normalized_name = ent.text.strip().lower()
        entities.append((normalized_name, ent.label_))

    return entities


class BaselineRAG:
    """Baseline RAG ç³»ç»Ÿ"""

    def __init__(self, faiss_index, chunks, client, config):
        self.faiss_index = faiss_index
        self.chunks = chunks
        self.client = client
        self.config = config

    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        response = self.client.embeddings.create(
            model=self.config['embedding']['model'],
            input=[query]
        )
        query_vector = np.array([response.data[0].embedding], dtype='float32')

        if self.config['faiss']['normalize_vectors']:
            faiss.normalize_L2(query_vector)

        distances, indices = self.faiss_index.search(query_vector, top_k)
        results = [self.chunks[idx] for idx in indices[0]]
        return results

    def answer(self, query: str, retrieved_docs: List[Dict]) -> str:
        """åŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ"""
        context = "\n\n".join([
            f"Document {i+1} ({doc['title']}):\n{doc['text']}"
            for i, doc in enumerate(retrieved_docs)
        ])

        prompt = f"""Answer the question based on the provided context.

Context:
{context}

Question: {query}

Answer (be concise):"""

        response = self.client.chat.completions.create(
            model=self.config['baseline_rag']['llm_model'],
            messages=[{"role": "user", "content": prompt}],
            temperature=self.config['baseline_rag']['llm_temperature'],
            max_completion_tokens=self.config['baseline_rag']['llm_max_tokens']
        )

        return response.choices[0].message.content.strip()


class HippoRAG:
    """HippoRAG ç³»ç»Ÿ"""

    def __init__(self, faiss_index, chunks, kg, pagerank_scores, client, config, nlp):
        self.faiss_index = faiss_index
        self.chunks = chunks
        self.kg = kg
        self.pagerank_scores = pagerank_scores
        self.client = client
        self.config = config
        self.nlp = nlp

        self.chunk_id_to_idx = {chunk['chunk_id']: i for i, chunk in enumerate(chunks)}

    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """ä½¿ç”¨ KG + PPR æ£€ç´¢"""
        initial_k = self.config['hipporag']['retrieval']['initial_k']

        response = self.client.embeddings.create(
            model=self.config['embedding']['model'],
            input=[query]
        )
        query_vector = np.array([response.data[0].embedding], dtype='float32')

        if self.config['faiss']['normalize_vectors']:
            faiss.normalize_L2(query_vector)

        distances, indices = self.faiss_index.search(query_vector, initial_k)

        query_entities = extract_entities_spacy(query, self.nlp)
        query_entity_ids = [f"entity_{name.replace(' ', '_')}" for name, _ in query_entities]
        query_entity_ids = [eid for eid in query_entity_ids if self.kg.has_node(eid)]

        if query_entity_ids:
            personalization = {node: 0.0 for node in self.kg.nodes}
            for eid in query_entity_ids:
                personalization[eid] = 1.0 / len(query_entity_ids)

            ppr_scores = nx.pagerank(
                self.kg,
                alpha=self.config['hipporag']['pagerank']['damping_factor'],
                personalization=personalization,
                max_iter=self.config['hipporag']['pagerank']['max_iterations']
            )
        else:
            ppr_scores = self.pagerank_scores

        candidate_chunks = [self.chunks[idx] for idx in indices[0]]

        reranked = []
        for chunk, distance in zip(candidate_chunks, distances[0]):
            chunk_id = chunk['chunk_id']
            ppr_score = ppr_scores.get(chunk_id, 0.0)
            retrieval_score = 1.0 / (1.0 + float(distance))
            combined_score = 0.5 * ppr_score + 0.5 * retrieval_score

            reranked.append({
                "chunk": chunk,
                "ppr_score": ppr_score,
                "retrieval_score": retrieval_score,
                "combined_score": combined_score
            })

        reranked.sort(key=lambda x: x['combined_score'], reverse=True)
        return [item['chunk'] for item in reranked[:top_k]]

    def answer(self, query: str, retrieved_docs: List[Dict]) -> str:
        """åŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆï¼ˆä¸ Baseline ç›¸åŒï¼‰"""
        context = "\n\n".join([
            f"Document {i+1} ({doc['title']}):\n{doc['text']}"
            for i, doc in enumerate(retrieved_docs)
        ])

        prompt = f"""Answer the question based on the provided context.

Context:
{context}

Question: {query}

Answer (be concise):"""

        response = self.client.chat.completions.create(
            model=self.config['hipporag']['retrieval']['llm_model'],
            messages=[{"role": "user", "content": prompt}],
            temperature=self.config['hipporag']['retrieval']['llm_temperature'],
            max_completion_tokens=self.config['hipporag']['retrieval']['llm_max_tokens']
        )

        return response.choices[0].message.content.strip()


def run_single_experiment(method_name: str, retriever, question: Dict) -> Dict:
    """è¿è¡Œå•ä¸ªé—®é¢˜çš„å®éªŒ"""
    query = question['question']
    gold_answer = question['answer']

    start_time = time.time()

    try:
        retrieved_docs = retriever.retrieve(query)
        predicted_answer = retriever.answer(query, retrieved_docs)
        latency = time.time() - start_time

        return {
            "question_id": question['id'],
            "question": query,
            "gold_answer": gold_answer,
            "predicted_answer": predicted_answer,
            "retrieved_docs": [doc['chunk_id'] for doc in retrieved_docs],
            "latency": latency,
            "success": True,
            "error": None
        }

    except Exception as e:
        return {
            "question_id": question['id'],
            "question": query,
            "gold_answer": gold_answer,
            "predicted_answer": "",
            "retrieved_docs": [],
            "latency": time.time() - start_time,
            "success": False,
            "error": str(e)
        }


def _load_checkpoint(checkpoint_path: Path) -> List[Dict]:
    if checkpoint_path.exists():
        with open(checkpoint_path, 'r') as f:
            data = json.load(f)
        if isinstance(data, list):
            return data
    return []


def _save_checkpoint(checkpoint_path: Path, results: List[Dict]) -> None:
    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
    with open(checkpoint_path, 'w') as f:
        json.dump(results, indent=2, fp=f)


def run_experiment(
    method_name: str,
    retriever,
    validation_set: List[Dict],
    config: Dict,
    checkpoint_path: Path,
) -> List[Dict]:
    """è¿è¡Œå®Œæ•´å®éªŒ"""
    print(f"\n{'='*60}")
    print(f"è¿è¡Œå®éªŒ: {method_name}")
    print(f"{'='*60}")

    results = _load_checkpoint(checkpoint_path)
    processed_ids = {r.get('question_id') for r in results if r.get('question_id')}

    max_workers = config['api']['max_concurrent_requests']
    total = len(validation_set)
    pending = [q for q in validation_set if q.get('id') not in processed_ids]
    if not pending:
        print(f"âœ… å·²å®Œæˆï¼Œæ— éœ€é‡å¤è¿è¡Œ: {method_name}")
        return results

    checkpoint_every = 20

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for question in pending:
            future = executor.submit(run_single_experiment, method_name, retriever, question)
            futures.append(future)

        with tqdm(total=total, initial=len(processed_ids), desc=f"{method_name} è¿›åº¦") as pbar:
            last_percent = int((pbar.n / pbar.total) * 100) if pbar.total else 0
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
                pbar.update(1)

                if len(results) % checkpoint_every == 0:
                    _save_checkpoint(checkpoint_path, results)

                percent = int((pbar.n / pbar.total) * 100) if pbar.total else 0
                if percent > last_percent:
                    print(f"{method_name} è¿›åº¦: {percent}% ({pbar.n}/{pbar.total})")
                    last_percent = percent

    success_count = sum(1 for r in results if r['success'])
    print(f"\nâœ… å®éªŒå®Œæˆï¼æˆåŠŸç‡: {success_count}/{len(results)}")

    _save_checkpoint(checkpoint_path, results)
    return results


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("HippoRAG å¯¹æ¯”å®éªŒï¼ˆHNSWï¼‰")
    print("=" * 80)

    config = load_config()

    print("\nğŸ“š åŠ è½½æ•°æ®...")
    validation_set = load_validation_set()
    print(f"âœ… éªŒè¯é›†: {len(validation_set)} ä¸ªé—®é¢˜")

    is_full = len(validation_set) > 500
    suffix = "_full" if is_full else ""

    baseline_checkpoint = project_root / "results" / f"checkpoint_baseline_hnsw_{len(validation_set)}.json"
    hipporag_checkpoint = project_root / "results" / f"checkpoint_hipporag_hnsw_{len(validation_set)}.json"

    print("\nğŸ“š åŠ è½½ç´¢å¼•...")
    faiss_index, chunks = load_hnsw_indices(config)
    print(f"âœ… HNSW ç´¢å¼•: {faiss_index.ntotal:,} ä¸ªå‘é‡")

    kg, pagerank_scores = load_hipporag_kg()
    print(f"âœ… çŸ¥è¯†å›¾è°±: {kg.number_of_nodes():,} ä¸ªèŠ‚ç‚¹")

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    nlp = spacy.load("en_core_web_sm")

    print("\n" + "="*80)
    print("å®éªŒ 1/2: Baseline RAGï¼ˆHNSWï¼‰")
    print("="*80)

    baseline_rag = BaselineRAG(faiss_index, chunks, client, config)
    baseline_results = run_experiment(
        "Baseline-RAG-HNSW",
        baseline_rag,
        validation_set,
        config,
        baseline_checkpoint,
    )

    results_dir = project_root / "results" / f"baseline_hnsw{suffix}"
    results_dir.mkdir(parents=True, exist_ok=True)
    with open(results_dir / "predictions.json", 'w') as f:
        json.dump(baseline_results, indent=2, fp=f)

    print("\n" + "="*80)
    print("å®éªŒ 2/2: HippoRAGï¼ˆHNSWï¼‰")
    print("="*80)

    hipporag = HippoRAG(faiss_index, chunks, kg, pagerank_scores, client, config, nlp)
    hipporag_results = run_experiment(
        "HippoRAG-HNSW",
        hipporag,
        validation_set,
        config,
        hipporag_checkpoint,
    )

    results_dir = project_root / "results" / f"hipporag_hnsw{suffix}"
    results_dir.mkdir(parents=True, exist_ok=True)
    with open(results_dir / "predictions.json", 'w') as f:
        json.dump(hipporag_results, indent=2, fp=f)

    print("\n" + "="*80)
    print("âœ… HNSW å®éªŒå®Œæˆï¼")
    print("="*80)


if __name__ == "__main__":
    main()
