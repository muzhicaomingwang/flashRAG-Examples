#!/usr/bin/env python3
"""
HotpotQA æ•°æ®é›†ä¸‹è½½å’Œé¢„å¤„ç†è„šæœ¬

åŠŸèƒ½ï¼š
1. ä» HuggingFace datasets ä¸‹è½½ HotpotQA
2. æå– corpusï¼ˆæ–‡æ¡£é›†åˆï¼‰
3. é‡‡æ ·éªŒè¯é›†ï¼ˆ500ä¸ªé—®é¢˜ï¼‰
4. ä¿å­˜åˆ° data/raw ç›®å½•
"""

import os
import sys
import json
import random
from pathlib import Path
from typing import Dict, List
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml
from datasets import load_dataset


def load_config() -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = project_root / "configs" / "experiment_config.yaml"
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def download_hotpotqa(config: Dict) -> None:
    """ä¸‹è½½ HotpotQA æ•°æ®é›†"""
    print("ğŸ“¥ ä¸‹è½½ HotpotQA æ•°æ®é›†...")

    # åŠ è½½æ•°æ®é›†ï¼ˆåªåŠ è½½ validation splitï¼‰
    dataset = load_dataset("hotpot_qa", "fullwiki", split="validation")

    print(f"âœ… ä¸‹è½½å®Œæˆï¼æ€»è®¡ {len(dataset)} ä¸ªæ ·æœ¬")

    return dataset


def extract_corpus(dataset) -> List[Dict]:
    """ä»æ•°æ®é›†ä¸­æå–æ–‡æ¡£è¯­æ–™åº“"""
    print("\nğŸ“š æå–æ–‡æ¡£è¯­æ–™åº“...")

    corpus = {}

    for example in tqdm(dataset, desc="å¤„ç†æ ·æœ¬"):
        # HotpotQA çš„ context åŒ…å«å¤šä¸ªæ®µè½
        # context æ ¼å¼: [title, [sent1, sent2, ...]]
        contexts = example['context']

        for title, sentences in zip(contexts['title'], contexts['sentences']):
            # ä½¿ç”¨ title ä½œä¸ºæ–‡æ¡£ ID
            doc_id = title.replace(" ", "_")

            # å¦‚æœæ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡
            if doc_id in corpus:
                continue

            # åˆå¹¶å¥å­æˆå®Œæ•´æ–‡æ¡£
            full_text = " ".join(sentences)

            corpus[doc_id] = {
                "id": doc_id,
                "title": title,
                "text": full_text
            }

    corpus_list = list(corpus.values())
    print(f"âœ… æå–å®Œæˆï¼æ€»è®¡ {len(corpus_list)} ä¸ªå”¯ä¸€æ–‡æ¡£")

    return corpus_list


def sample_validation_set(dataset, config: Dict) -> List[Dict]:
    """é‡‡æ ·éªŒè¯é›†"""
    print("\nğŸ² é‡‡æ ·éªŒè¯é›†...")

    max_samples = config['dataset']['max_samples']
    random_seed = config['dataset']['random_seed']

    # è®¾ç½®éšæœºç§å­
    random.seed(random_seed)

    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶é‡‡æ ·
    all_examples = list(dataset)

    if len(all_examples) > max_samples:
        sampled = random.sample(all_examples, max_samples)
    else:
        sampled = all_examples

    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    validation_set = []
    for idx, example in enumerate(sampled):
        validation_set.append({
            "id": f"hotpotqa_dev_{idx}",
            "question": example['question'],
            "answer": example['answer'],
            "type": example['type'],
            "level": example['level'],
            "supporting_facts": example['supporting_facts']
        })

    print(f"âœ… é‡‡æ ·å®Œæˆï¼éªŒè¯é›†å¤§å°: {len(validation_set)}")

    return validation_set


def save_data(corpus: List[Dict], validation_set: List[Dict]) -> None:
    """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
    print("\nğŸ’¾ ä¿å­˜æ•°æ®...")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    raw_dir = project_root / "data" / "raw"
    raw_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜ corpus
    corpus_path = raw_dir / "hotpotqa_corpus.jsonl"
    with open(corpus_path, 'w', encoding='utf-8') as f:
        for doc in corpus:
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')

    print(f"âœ… Corpus å·²ä¿å­˜: {corpus_path}")

    # ä¿å­˜ validation set
    val_path = raw_dir / "hotpotqa_validation.jsonl"
    with open(val_path, 'w', encoding='utf-8') as f:
        for example in validation_set:
            f.write(json.dumps(example, ensure_ascii=False) + '\n')

    print(f"âœ… éªŒè¯é›†å·²ä¿å­˜: {val_path}")

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "corpus_size": len(corpus),
        "validation_size": len(validation_set),
        "avg_doc_length": sum(len(doc['text'].split()) for doc in corpus) / len(corpus),
        "avg_question_length": sum(len(q['question'].split()) for q in validation_set) / len(validation_set)
    }

    stats_path = raw_dir / "dataset_stats.json"
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, indent=2, fp=f)

    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_path}")
    print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  - æ–‡æ¡£æ•°é‡: {stats['corpus_size']:,}")
    print(f"  - éªŒè¯é›†å¤§å°: {stats['validation_size']}")
    print(f"  - å¹³å‡æ–‡æ¡£é•¿åº¦: {stats['avg_doc_length']:.1f} è¯")
    print(f"  - å¹³å‡é—®é¢˜é•¿åº¦: {stats['avg_question_length']:.1f} è¯")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("HotpotQA æ•°æ®é›†ä¸‹è½½å’Œé¢„å¤„ç†")
    print("=" * 60)

    # åŠ è½½é…ç½®
    config = load_config()

    # ä¸‹è½½æ•°æ®é›†
    dataset = download_hotpotqa(config)

    # æå–è¯­æ–™åº“
    corpus = extract_corpus(dataset)

    # é‡‡æ ·éªŒè¯é›†
    validation_set = sample_validation_set(dataset, config)

    # ä¿å­˜æ•°æ®
    save_data(corpus, validation_set)

    print("\n" + "=" * 60)
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("   python scripts/02_build_baseline.py")


if __name__ == "__main__":
    main()
