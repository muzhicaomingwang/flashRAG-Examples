#!/usr/bin/env python3
"""
HippoRAG çŸ¥è¯†å›¾è°±æ„å»ºè„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½åˆ†å—åçš„æ–‡æ¡£
2. ä½¿ç”¨ SpaCy è¿›è¡Œå®ä½“è¯†åˆ«ï¼ˆNERï¼‰
3. ä½¿ç”¨ä¾å­˜å¥æ³•åˆ†ææå–å…³ç³»ï¼ˆREï¼‰
4. æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆNetworkXï¼‰
5. è®¡ç®— Personalized PageRank
"""

import os
import sys
import json
import pickle
import random
from pathlib import Path
from typing import List, Dict, Tuple, Set
from collections import defaultdict
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml
import spacy
import networkx as nx
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def load_config() -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = project_root / "configs" / "experiment_config.yaml"
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def load_chunks() -> List[Dict]:
    """åŠ è½½åˆ†å—åçš„æ–‡æ¡£"""
    print("ğŸ“š åŠ è½½æ–‡æ¡£å—...")
    chunks_path = project_root / "data" / "processed" / "hotpotqa_chunks.jsonl"

    chunks = []
    with open(chunks_path, 'r', encoding='utf-8') as f:
        for line in f:
            chunks.append(json.loads(line))

    print(f"âœ… åŠ è½½å®Œæˆï¼æ€»è®¡ {len(chunks)} ä¸ªå—")
    return chunks


def sample_chunks(chunks: List[Dict], sampling_ratio: float, random_seed: int = 42) -> List[Dict]:
    """é‡‡æ ·æ–‡æ¡£å—ï¼ˆèŠ‚çœæˆæœ¬ï¼‰"""
    print(f"\nğŸ² é‡‡æ · {sampling_ratio*100:.0f}% çš„æ–‡æ¡£å—...")

    random.seed(random_seed)
    n_samples = int(len(chunks) * sampling_ratio)

    sampled_chunks = random.sample(chunks, n_samples)

    print(f"âœ… é‡‡æ ·å®Œæˆï¼{len(chunks)} â†’ {len(sampled_chunks)} ä¸ªå—")

    return sampled_chunks


def extract_entities_spacy(text: str, nlp) -> List[Tuple[str, str]]:
    """ä½¿ç”¨ SpaCy æå–å®ä½“"""
    doc = nlp(text)

    entities = []
    for ent in doc.ents:
        # å½’ä¸€åŒ–å®ä½“åç§°
        normalized_name = ent.text.strip().lower()
        entities.append((normalized_name, ent.label_))

    return entities


def extract_relations_spacy(text: str, nlp) -> List[Tuple[str, str, str]]:
    """ä½¿ç”¨ SpaCy ä¾å­˜å¥æ³•åˆ†ææå–å…³ç³»"""
    doc = nlp(text)

    relations = []

    for token in doc:
        # æå–ä¸»è°“å®¾å…³ç³»
        if token.dep_ in ["nsubj", "nsubjpass"]:
            subject = token.text.lower()
            verb = token.head.lemma_
            # æŸ¥æ‰¾å®¾è¯­
            for child in token.head.children:
                if child.dep_ in ["dobj", "attr", "prep"]:
                    obj = child.text.lower()
                    relations.append((subject, verb, obj))

        # æå–ä¿®é¥°å…³ç³»
        elif token.dep_ in ["amod", "compound"]:
            relations.append((token.head.text.lower(), "is_a", token.text.lower()))

    return relations


def build_knowledge_graph(chunks: List[Dict], config: Dict) -> nx.DiGraph:
    """æ„å»ºçŸ¥è¯†å›¾è°±"""
    print("\nğŸ”¨ æ„å»ºçŸ¥è¯†å›¾è°±...")

    # åŠ è½½ SpaCy æ¨¡å‹
    nlp = spacy.load(config['hipporag']['entity_extraction']['spacy_model'])

    # åˆå§‹åŒ–å›¾
    kg = nx.DiGraph()

    # å®ä½“ç»Ÿè®¡
    entity_counts = defaultdict(int)
    relation_counts = defaultdict(int)

    # å¤„ç†æ¯ä¸ªæ–‡æ¡£å—
    for chunk in tqdm(chunks, desc="æå–å®ä½“å’Œå…³ç³»"):
        chunk_id = chunk['chunk_id']
        text = chunk['text']

        # æ·»åŠ æ–‡æ¡£å—èŠ‚ç‚¹
        kg.add_node(chunk_id, type="chunk", text=text, title=chunk['title'])

        # æå–å®ä½“
        entities = extract_entities_spacy(text, nlp)

        # å»é‡
        unique_entities = list(set(entities))

        # é™åˆ¶æ¯ä¸ªå—çš„æœ€å¤§å®ä½“æ•°
        max_entities = config['hipporag']['entity_extraction']['max_entities_per_chunk']
        if len(unique_entities) > max_entities:
            unique_entities = unique_entities[:max_entities]

        # æ·»åŠ å®ä½“èŠ‚ç‚¹å’Œchunk-entityè¾¹
        for entity_name, entity_type in unique_entities:
            entity_id = f"entity_{entity_name.replace(' ', '_')}"

            # æ·»åŠ å®ä½“èŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if not kg.has_node(entity_id):
                kg.add_node(entity_id, type="entity", name=entity_name, entity_type=entity_type)

            # æ·»åŠ  chunk -> entity è¾¹
            kg.add_edge(chunk_id, entity_id, relation="contains")

            # ç»Ÿè®¡
            entity_counts[entity_name] += 1

        # æå–å…³ç³»
        relations = extract_relations_spacy(text, nlp)

        # é™åˆ¶æ¯ä¸ªå—çš„æœ€å¤§å…³ç³»æ•°
        max_relations = config['hipporag']['relation_extraction']['max_relations_per_chunk']
        if len(relations) > max_relations:
            relations = relations[:max_relations]

        # æ·»åŠ å®ä½“é—´å…³ç³»
        for subj, rel, obj in relations:
            subj_id = f"entity_{subj.replace(' ', '_')}"
            obj_id = f"entity_{obj.replace(' ', '_')}"

            # åªæ·»åŠ å›¾ä¸­å·²å­˜åœ¨å®ä½“çš„å…³ç³»
            if kg.has_node(subj_id) and kg.has_node(obj_id):
                kg.add_edge(subj_id, obj_id, relation=rel)
                relation_counts[rel] += 1

    print(f"\nâœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
    print(f"   - èŠ‚ç‚¹æ€»æ•°: {kg.number_of_nodes():,}")
    print(f"   - è¾¹æ€»æ•°: {kg.number_of_edges():,}")
    print(f"   - å”¯ä¸€å®ä½“æ•°: {len([n for n in kg.nodes if kg.nodes[n]['type'] == 'entity']):,}")
    print(f"   - å”¯ä¸€å…³ç³»ç±»å‹: {len(relation_counts):,}")

    # æ˜¾ç¤ºæœ€å¸¸è§çš„å®ä½“å’Œå…³ç³»
    print(f"\nğŸ“Š Top-10 æœ€å¸¸è§å®ä½“:")
    for entity, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"   - {entity}: {count}")

    print(f"\nğŸ“Š Top-10 æœ€å¸¸è§å…³ç³»:")
    for rel, count in sorted(relation_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"   - {rel}: {count}")

    return kg


def compute_pagerank(kg: nx.DiGraph, config: Dict) -> Dict[str, float]:
    """è®¡ç®— Personalized PageRank"""
    print("\nğŸ”¨ è®¡ç®— Personalized PageRank...")

    pr_config = config['hipporag']['pagerank']

    # è®¡ç®— PageRankï¼ˆæ— ä¸ªæ€§åŒ–ï¼Œå…ˆè®¡ç®—å…¨å±€é‡è¦æ€§ï¼‰
    pagerank_scores = nx.pagerank(
        kg,
        alpha=pr_config['damping_factor'],
        max_iter=pr_config['max_iterations'],
        tol=pr_config['convergence_threshold']
    )

    print(f"âœ… PageRank è®¡ç®—å®Œæˆï¼")

    # æ˜¾ç¤ºæœ€é‡è¦çš„èŠ‚ç‚¹
    print(f"\nğŸ“Š Top-10 æœ€é‡è¦çš„å®ä½“:")
    entity_scores = {n: s for n, s in pagerank_scores.items() if kg.nodes[n]['type'] == 'entity'}
    for node_id, score in sorted(entity_scores.items(), key=lambda x: x[1], reverse=True)[:10]:
        entity_name = kg.nodes[node_id]['name']
        print(f"   - {entity_name}: {score:.6f}")

    return pagerank_scores


def save_knowledge_graph(kg: nx.DiGraph, pagerank_scores: Dict, config: Dict) -> None:
    """ä¿å­˜çŸ¥è¯†å›¾è°±"""
    print("\nğŸ’¾ ä¿å­˜çŸ¥è¯†å›¾è°±...")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    kg_dir = project_root / "data" / "knowledge_graphs"
    kg_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜å›¾è°±
    kg_path = kg_dir / "hotpotqa_kg.gpickle"
    nx.write_gpickle(kg, str(kg_path))
    print(f"âœ… çŸ¥è¯†å›¾è°±å·²ä¿å­˜: {kg_path}")

    # ä¿å­˜ PageRank åˆ†æ•°
    pr_path = kg_dir / "hotpotqa_pagerank.pkl"
    with open(pr_path, 'wb') as f:
        pickle.dump(pagerank_scores, f)
    print(f"âœ… PageRank åˆ†æ•°å·²ä¿å­˜: {pr_path}")

    # ä¿å­˜å›¾è°±ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "num_nodes": kg.number_of_nodes(),
        "num_edges": kg.number_of_edges(),
        "num_chunks": len([n for n in kg.nodes if kg.nodes[n]['type'] == 'chunk']),
        "num_entities": len([n for n in kg.nodes if kg.nodes[n]['type'] == 'entity']),
        "avg_degree": sum(dict(kg.degree()).values()) / kg.number_of_nodes(),
        "density": nx.density(kg)
    }

    stats_path = kg_dir / "hotpotqa_kg_stats.json"
    with open(stats_path, 'w') as f:
        json.dump(stats, indent=2, fp=f)
    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("HippoRAG çŸ¥è¯†å›¾è°±æ„å»º")
    print("=" * 60)

    # åŠ è½½é…ç½®
    config = load_config()

    # åŠ è½½æ–‡æ¡£å—
    chunks = load_chunks()

    # é‡‡æ ·æ–‡æ¡£å—ï¼ˆèŠ‚çœæˆæœ¬ï¼‰
    sampling_ratio = config['hipporag']['sampling_ratio']
    sampled_chunks = sample_chunks(chunks, sampling_ratio)

    # æ„å»ºçŸ¥è¯†å›¾è°±
    kg = build_knowledge_graph(sampled_chunks, config)

    # è®¡ç®— PageRank
    pagerank_scores = compute_pagerank(kg, config)

    # ä¿å­˜çŸ¥è¯†å›¾è°±
    save_knowledge_graph(kg, pagerank_scores, config)

    print("\n" + "=" * 60)
    print("âœ… HippoRAG çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("   python scripts/05_run_experiments.py")


if __name__ == "__main__":
    main()
