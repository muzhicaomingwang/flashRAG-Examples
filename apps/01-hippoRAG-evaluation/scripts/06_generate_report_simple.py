#!/usr/bin/env python3
"""
è¯„ä¼°æŠ¥å‘Šç”Ÿæˆè„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
"""

import json
import string
import re
from pathlib import Path
import numpy as np

project_root = Path(__file__).parent.parent


def normalize_answer(answer: str) -> str:
    """å½’ä¸€åŒ–ç­”æ¡ˆ"""
    answer = answer.lower()
    answer = answer.translate(str.maketrans('', '', string.punctuation))
    answer = re.sub(r'\s+', ' ', answer).strip()
    return answer


def compute_f1(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—F1åˆ†æ•°"""
    pred_tokens = normalize_answer(prediction).split()
    gold_tokens = normalize_answer(ground_truth).split()

    if len(pred_tokens) == 0 or len(gold_tokens) == 0:
        return 0.0

    common = set(pred_tokens) & set(gold_tokens)

    if len(common) == 0:
        return 0.0

    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(gold_tokens)
    f1 = 2 * (precision * recall) / (precision + recall)

    return f1


def compute_exact_match(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…"""
    return 1.0 if normalize_answer(prediction) == normalize_answer(ground_truth) else 0.0


def evaluate_results(results: list) -> dict:
    """è¯„ä¼°ç»“æœ"""
    f1_scores = []
    em_scores = []
    latencies = []
    success_count = 0

    for result in results:
        if result['success']:
            f1 = compute_f1(result['predicted_answer'], result['gold_answer'])
            f1_scores.append(f1)

            em = compute_exact_match(result['predicted_answer'], result['gold_answer'])
            em_scores.append(em)

            latencies.append(result['latency'])
            success_count += 1

    metrics = {
        "num_questions": len(results),
        "success_count": success_count,
        "success_rate": success_count / len(results) if results else 0.0,
        "f1": {
            "mean": float(np.mean(f1_scores)) if f1_scores else 0.0,
            "std": float(np.std(f1_scores)) if f1_scores else 0.0
        },
        "exact_match": {
            "mean": float(np.mean(em_scores)) if em_scores else 0.0,
            "std": float(np.std(em_scores)) if em_scores else 0.0
        },
        "latency": {
            "mean": float(np.mean(latencies)) if latencies else 0.0,
            "std": float(np.std(latencies)) if latencies else 0.0,
            "median": float(np.median(latencies)) if latencies else 0.0
        }
    }

    return metrics


def main():
    print("=" * 60)
    print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)

    # åŠ è½½ç»“æœ
    print("\nğŸ“š åŠ è½½å®éªŒç»“æœ...")

    baseline_path = project_root / "results" / "baseline" / "predictions.json"
    hipporag_path = project_root / "results" / "hipporag" / "predictions.json"

    with open(baseline_path, 'r') as f:
        baseline_results = json.load(f)
    print(f"âœ… Baseline: {len(baseline_results)} é—®é¢˜")

    with open(hipporag_path, 'r') as f:
        hipporag_results = json.load(f)
    print(f"âœ… HippoRAG: {len(hipporag_results)} é—®é¢˜")

    # è¯„ä¼°
    print("\nğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")

    baseline_metrics = evaluate_results(baseline_results)
    hipporag_metrics = evaluate_results(hipporag_results)

    print(f"\nBaseline RAG:")
    print(f"  - F1: {baseline_metrics['f1']['mean']:.4f} Â± {baseline_metrics['f1']['std']:.4f}")
    print(f"  - EM: {baseline_metrics['exact_match']['mean']:.4f}")
    print(f"  - å»¶è¿Ÿ: {baseline_metrics['latency']['mean']:.2f}s")

    print(f"\nHippoRAG:")
    print(f"  - F1: {hipporag_metrics['f1']['mean']:.4f} Â± {hipporag_metrics['f1']['std']:.4f}")
    print(f"  - EM: {hipporag_metrics['exact_match']['mean']:.4f}")
    print(f"  - å»¶è¿Ÿ: {hipporag_metrics['latency']['mean']:.2f}s")

    # è®¡ç®—æå‡
    f1_improvement = ((hipporag_metrics['f1']['mean'] - baseline_metrics['f1']['mean']) / baseline_metrics['f1']['mean']) * 100 if baseline_metrics['f1']['mean'] > 0 else 0

    print(f"\nğŸ“ˆ æ€§èƒ½æå‡:")
    print(f"  - F1æå‡: {f1_improvement:+.2f}%")

    # ä¿å­˜æŒ‡æ ‡
    all_metrics = {
        'Baseline-RAG': baseline_metrics,
        'HippoRAG': hipporag_metrics
    }

    metrics_path = project_root / "results" / "evaluation_metrics.json"
    with open(metrics_path, 'w') as f:
        json.dump(all_metrics, indent=2, fp=f)
    print(f"\nâœ… æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")

    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    table = "# HippoRAG vs Baseline RAG å®éªŒç»“æœ\n\n"
    table += "## æ€§èƒ½å¯¹æ¯”\n\n"
    table += "| æ–¹æ³• | F1 Score | Exact Match | å¹³å‡å»¶è¿Ÿ (ç§’) | æˆåŠŸç‡ |\n"
    table += "|------|----------|-------------|--------------|--------|\n"
    table += f"| **Baseline-RAG** | {baseline_metrics['f1']['mean']:.4f} Â± {baseline_metrics['f1']['std']:.4f} | {baseline_metrics['exact_match']['mean']:.4f} | {baseline_metrics['latency']['mean']:.2f} | {baseline_metrics['success_rate']:.2%} |\n"
    table += f"| **HippoRAG** | {hipporag_metrics['f1']['mean']:.4f} Â± {hipporag_metrics['f1']['std']:.4f} | {hipporag_metrics['exact_match']['mean']:.4f} | {hipporag_metrics['latency']['mean']:.2f} | {hipporag_metrics['success_rate']:.2%} |\n"

    table += f"\n## æ€§èƒ½æå‡\n\n"
    table += f"- **F1 Scoreæå‡:** {f1_improvement:+.2f}%\n"

    if f1_improvement > 10:
        table += f"- **ç»“è®º:** âœ… HippoRAGåœ¨å¤šè·³é—®ç­”ä¸Šæœ‰æ˜¾è‘—æå‡\n"
    elif f1_improvement > 0:
        table += f"- **ç»“è®º:** âœ… HippoRAGæœ‰é€‚åº¦æå‡\n"
    else:
        table += f"- **ç»“è®º:** âš ï¸ HippoRAGæœªå¸¦æ¥æ˜æ˜¾æ”¹è¿›\n"

    table_path = project_root / "results" / "comparison_table.md"
    with open(table_path, 'w') as f:
        f.write(table)
    print(f"âœ… å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜: {table_path}")

    print("\n" + "=" * 60)
    print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)
    print(f"\n{table}")


if __name__ == "__main__":
    main()
