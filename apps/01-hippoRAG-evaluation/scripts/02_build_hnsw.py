#!/usr/bin/env python3
"""
HNSW ç´¢å¼•æ„å»ºè„šæœ¬

åŠŸèƒ½ï¼š
1. æ–‡æ¡£åˆ†å—ï¼ˆä¸ Baseline ä¿æŒä¸€è‡´ï¼‰
2. æ„å»º FAISS HNSW å‘é‡ç´¢å¼•
3. ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜ï¼ˆç‹¬ç«‹è·¯å¾„ï¼‰
"""

import os
import sys
import json
import pickle
from pathlib import Path
from typing import List, Dict
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml
import numpy as np
import faiss
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def load_config() -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = project_root / "configs" / "experiment_config.yaml"
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def load_corpus() -> List[Dict]:
    """åŠ è½½æ–‡æ¡£è¯­æ–™åº“ï¼ˆä½¿ç”¨é‡‡æ ·ç‰ˆæœ¬ä»¥èŠ‚çœå†…å­˜ï¼‰"""
    print("ğŸ“š åŠ è½½æ–‡æ¡£è¯­æ–™åº“...")
    sampled_path = project_root / "data" / "raw" / "hotpotqa_corpus_sampled.jsonl"
    full_path = project_root / "data" / "raw" / "hotpotqa_corpus.jsonl"

    corpus_path = sampled_path if sampled_path.exists() else full_path

    corpus = []
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line in f:
            corpus.append(json.loads(line))

    print(f"âœ… åŠ è½½å®Œæˆï¼æ€»è®¡ {len(corpus)} ä¸ªæ–‡æ¡£")
    if corpus_path == sampled_path:
        print("   ï¼ˆä½¿ç”¨é‡‡æ ·ç‰ˆæœ¬ä»¥èŠ‚çœå†…å­˜ï¼‰")
    return corpus


def load_existing_flat_index(config: Dict):
    """åŠ è½½ç°æœ‰ FlatL2 ç´¢å¼•ä¸æ–‡æ¡£æ˜ å°„ï¼ˆç”¨äºå¤ç”¨å‘é‡ï¼‰"""
    faiss_index_path = project_root / config['faiss']['persist_path']
    doc_mapping_path = project_root / config['faiss']['doc_mapping_path']

    if not faiss_index_path.exists() or not doc_mapping_path.exists():
        return None, None

    print("ğŸ“¦ å‘ç°ç°æœ‰ FlatL2 ç´¢å¼•ï¼Œç›´æ¥å¤ç”¨å‘é‡...")
    faiss_index = faiss.read_index(str(faiss_index_path))
    with open(doc_mapping_path, 'rb') as f:
        chunks = pickle.load(f)

    return faiss_index, chunks


def chunk_documents(corpus: List[Dict], config: Dict) -> List[Dict]:
    """æ–‡æ¡£åˆ†å—ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨å­—ç¬¦æ•°è¿‘ä¼¼ä»£æ›¿tokenæ•°ï¼‰"""
    print("\nâœ‚ï¸  æ–‡æ¡£åˆ†å—...")

    chunk_config = config['document_processing']
    chunk_size_chars = chunk_config['chunk_size'] * 4
    chunk_overlap_chars = chunk_config['chunk_overlap'] * 4

    chunks = []

    for doc in tqdm(corpus, desc="åˆ†å—è¿›åº¦"):
        doc_text = doc['text']
        text_length = len(doc_text)

        if text_length <= chunk_size_chars:
            chunks.append({
                "chunk_id": f"{doc['id']}_0",
                "doc_id": doc['id'],
                "title": doc['title'],
                "text": doc_text,
                "chunk_index": 0
            })
            continue

        start = 0
        chunk_index = 0

        while start < text_length:
            end = min(start + chunk_size_chars, text_length)
            chunk_text = doc_text[start:end]

            chunks.append({
                "chunk_id": f"{doc['id']}_{chunk_index}",
                "doc_id": doc['id'],
                "title": doc['title'],
                "text": chunk_text,
                "chunk_index": chunk_index
            })

            start = end - chunk_overlap_chars
            chunk_index += 1

    print(f"âœ… åˆ†å—å®Œæˆï¼æ€»è®¡ {len(chunks)} ä¸ªå—")
    print(f"   å¹³å‡æ¯æ–‡æ¡£ {len(chunks) / len(corpus):.1f} ä¸ªå—")

    return chunks


def build_hnsw_index(chunks: List[Dict], config: Dict):
    """æ„å»º FAISS HNSW ç´¢å¼•ï¼ˆä¼˜å…ˆå¤ç”¨ FlatL2 å‘é‡ï¼‰"""
    print("\nğŸ”¨ æ„å»º FAISS HNSW å‘é‡ç´¢å¼•...")

    flat_index, flat_chunks = load_existing_flat_index(config)
    if flat_index is not None and flat_chunks is not None:
        chunks = flat_chunks
        embeddings_array = flat_index.reconstruct_n(0, flat_index.ntotal)
        print(f"âœ… å¤ç”¨å‘é‡å®Œæˆï¼ç»´åº¦: {embeddings_array.shape}")
    else:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        embedding_config = config['embedding']
        model = embedding_config['model']
        batch_size = embedding_config['batch_size']

        texts = [chunk['text'] for chunk in chunks]

        all_embeddings = []
        for i in tqdm(range(0, len(texts), batch_size), desc="å‘é‡åŒ–è¿›åº¦"):
            batch_texts = texts[i:i + batch_size]
            response = client.embeddings.create(
                model=model,
                input=batch_texts
            )
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)

        embeddings_array = np.array(all_embeddings, dtype='float32')

        if config['faiss']['normalize_vectors']:
            faiss.normalize_L2(embeddings_array)

        print(f"âœ… å‘é‡åŒ–å®Œæˆï¼ç»´åº¦: {embeddings_array.shape}")

    hnsw_cfg = config['faiss_hnsw']
    dimension = embeddings_array.shape[1]
    m = int(hnsw_cfg['m'])

    print(f"ğŸ”¨ æ„å»º FAISS IndexHNSWFlat (M={m})...")
    index = faiss.IndexHNSWFlat(dimension, m, faiss.METRIC_L2)
    index.hnsw.efConstruction = int(hnsw_cfg['ef_construction'])
    index.add(embeddings_array)

    print(f"âœ… HNSW ç´¢å¼•æ„å»ºå®Œæˆï¼ç´¢å¼•åŒ…å« {index.ntotal} ä¸ªå‘é‡")

    return index, chunks


def save_indices(faiss_index, chunks: List[Dict], config: Dict) -> None:
    """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
    print("\nğŸ’¾ ä¿å­˜ç´¢å¼•...")

    faiss_dir = project_root / "data" / "indices" / "faiss"
    faiss_dir.mkdir(parents=True, exist_ok=True)

    faiss_index_path = project_root / config['faiss_hnsw']['persist_path']
    faiss.write_index(faiss_index, str(faiss_index_path))
    print(f"âœ… HNSW ç´¢å¼•å·²ä¿å­˜: {faiss_index_path}")

    doc_mapping_path = project_root / config['faiss_hnsw']['doc_mapping_path']
    with open(doc_mapping_path, 'wb') as f:
        pickle.dump(chunks, f)
    print(f"âœ… æ–‡æ¡£æ˜ å°„å·²ä¿å­˜: {doc_mapping_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("HNSW ç´¢å¼•æ„å»º")
    print("=" * 60)

    config = load_config()

    flat_index, flat_chunks = load_existing_flat_index(config)
    if flat_chunks is not None:
        chunks = flat_chunks
    else:
        corpus = load_corpus()
        chunks = chunk_documents(corpus, config)

    faiss_index, _ = build_hnsw_index(chunks, config)
    save_indices(faiss_index, chunks, config)

    print("\n" + "=" * 60)
    print("âœ… HNSW ç´¢å¼•æ„å»ºå®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("   python scripts/05_run_experiments_hnsw.py")


if __name__ == "__main__":
    main()
