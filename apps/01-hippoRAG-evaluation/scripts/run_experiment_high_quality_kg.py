#!/usr/bin/env python3
"""
ä½¿ç”¨é«˜è´¨é‡KGè¿è¡ŒHippoRAGå®éªŒ
å¯¹æ¯”æ ‡å‡†KG vs é«˜è´¨é‡KGçš„æ•ˆæœ
"""

import json
import pickle
import time
import os
import gc
from pathlib import Path
import numpy as np
import faiss
import spacy
import networkx as nx
from openai import OpenAI

print('='*80)
print('HippoRAGå®éªŒ - é«˜è´¨é‡çŸ¥è¯†å›¾è°±ç‰ˆæœ¬')
print('='*80)

# åŠ è½½æ•°æ®
print('\nğŸ“š åŠ è½½æ•°æ®å’Œç´¢å¼•...')

validation = []
with open('data/raw/hotpotqa_validation.jsonl', 'r') as f:
    for line in f:
        validation.append(json.loads(line))
print(f'âœ… éªŒè¯é›†: {len(validation)} é—®é¢˜')

# ä½¿ç”¨_fullç‰ˆæœ¬çš„ç´¢å¼•ï¼ˆä¿æŒä¸ä¹‹å‰ä¸€è‡´ï¼‰
faiss_index = faiss.read_index('data/indices/faiss/hotpotqa_full.index')
with open('data/indices/faiss/hotpotqa_full_docs.pkl', 'rb') as f:
    chunks = pickle.load(f)
print(f'âœ… FAISS: {faiss_index.ntotal:,} å‘é‡')

# ä½¿ç”¨é«˜è´¨é‡ç‰ˆæœ¬çš„KG
print('\nğŸ“Š åŠ è½½é«˜è´¨é‡KG...')
with open('data/knowledge_graphs/hotpotqa_kg_high_quality.gpickle', 'rb') as f:
    kg = pickle.load(f)
with open('data/knowledge_graphs/hotpotqa_pagerank_high_quality.pkl', 'rb') as f:
    pagerank_scores = pickle.load(f)

# ç»Ÿè®¡KG
entity_to_entity = sum(1 for u, v in kg.edges() if u.startswith('entity_') and v.startswith('entity_'))
print(f'âœ… çŸ¥è¯†å›¾è°±: {kg.number_of_nodes():,} èŠ‚ç‚¹, {kg.number_of_edges():,} è¾¹')
print(f'   Entityâ†’Entityå…³ç³»: {entity_to_entity:,} ({entity_to_entity/kg.number_of_edges()*100:.1f}%)')

# åˆå§‹åŒ–
client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))
nlp = spacy.load('en_core_web_sm')

# PPRç¼“å­˜ç±»ï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰
class PPRCache:
    def __init__(self, kg, global_pagerank):
        self.kg = kg
        self.global_pr = global_pagerank
        self.cache = {}
        self.hits = 0
        self.misses = 0

    def get_ppr(self, query_entity_ids):
        if not query_entity_ids:
            return self.global_pr

        cache_key = frozenset(query_entity_ids)
        if cache_key in self.cache:
            self.hits += 1
            return self.cache[cache_key]

        self.misses += 1
        personalization = {node: 0.0 for node in self.kg.nodes}
        for eid in query_entity_ids:
            personalization[eid] = 1.0 / len(query_entity_ids)

        ppr_scores = nx.pagerank(self.kg, alpha=0.85, personalization=personalization, max_iter=100)
        self.cache[cache_key] = ppr_scores
        return ppr_scores

    def get_stats(self):
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        return {
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': hit_rate,
            'cache_size': len(self.cache)
        }

# æ£€æŸ¥ç‚¹å‡½æ•°
def load_checkpoint(checkpoint_file):
    if Path(checkpoint_file).exists():
        with open(checkpoint_file, 'r') as f:
            return json.load(f)
    return []

def save_checkpoint(results, checkpoint_file):
    Path(checkpoint_file).parent.mkdir(parents=True, exist_ok=True)
    with open(checkpoint_file, 'w') as f:
        json.dump(results, f, indent=2)

def print_progress(current, total, start_time):
    if current == 0:
        return
    percent = current / total * 100
    elapsed = time.time() - start_time
    rate = elapsed / current
    remaining_min = (rate * (total - current)) / 60
    print(f'   [{current}/{total}] {percent:.1f}% - '
          f'ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ - '
          f'é¢„è®¡å‰©ä½™: {remaining_min:.1f}åˆ†é’Ÿ')

# è¿è¡ŒHippoRAGå®éªŒ
print('\n' + '='*80)
print('è¿è¡Œ HippoRAGï¼ˆé«˜è´¨é‡KGï¼‰')
print('='*80)

ppr_cache = PPRCache(kg, pagerank_scores)
results = load_checkpoint('results/checkpoint_hipporag_hq.json')
start_idx = len(results)

if start_idx > 0:
    print(f'ğŸ“‹ ä»checkpointæ¢å¤ï¼Œå·²å®Œæˆ: {start_idx}/{len(validation)}')

start_time = time.time()

for i in range(start_idx, len(validation)):
    question = validation[i]

    if i % 10 == 0 or i == start_idx:
        print_progress(i - start_idx, len(validation) - start_idx, start_time)

    query = question['question']
    gold_answer = question['answer']
    q_start_time = time.time()

    try:
        # å‘é‡åŒ–æŸ¥è¯¢
        resp = client.embeddings.create(model='text-embedding-3-small', input=[query])
        q_vec = np.array([resp.data[0].embedding], dtype='float32')
        faiss.normalize_L2(q_vec)

        # FAISSæ£€ç´¢ï¼ˆtop 20ï¼‰
        distances, indices = faiss_index.search(q_vec, 20)

        # æå–æŸ¥è¯¢å®ä½“
        query_doc = nlp(query)
        query_entities = [ent.text.lower().strip() for ent in query_doc.ents]
        query_entity_ids = [f"entity_{e.replace(' ', '_')}" for e in query_entities]
        query_entity_ids = [eid for eid in query_entity_ids if kg.has_node(eid)]

        # ä½¿ç”¨PPRç¼“å­˜
        ppr_scores = ppr_cache.get_ppr(query_entity_ids)

        # é‡æ–°æ’åº
        candidate_chunks = [chunks[idx] for idx in indices[0]]
        reranked = []

        for chunk, distance in zip(candidate_chunks, distances[0]):
            chunk_id = chunk['chunk_id']
            ppr_score = ppr_scores.get(chunk_id, 0.0)
            retrieval_score = 1.0 / (1.0 + float(distance))
            combined_score = 0.5 * ppr_score + 0.5 * retrieval_score
            reranked.append({'chunk': chunk, 'combined_score': combined_score})

        reranked.sort(key=lambda x: x['combined_score'], reverse=True)
        retrieved_docs = [item['chunk'] for item in reranked[:5]]

        # ç”Ÿæˆç­”æ¡ˆ
        context = '\n\n'.join([f"Doc {j+1}: {doc['text']}" for j, doc in enumerate(retrieved_docs)])
        prompt = f"""Answer the question based on the context.

Context:
{context}

Question: {query}

Answer (be concise):"""

        answer_resp = client.chat.completions.create(
            model='gpt-3.5-turbo',
            messages=[{'role': 'user', 'content': prompt}],
            temperature=0.0,
            max_tokens=256
        )

        predicted = answer_resp.choices[0].message.content.strip()

        results.append({
            'question_id': question['id'],
            'question': query,
            'gold_answer': gold_answer,
            'predicted_answer': predicted,
            'latency': time.time() - q_start_time,
            'query_entities': query_entities,
            'success': True
        })

    except Exception as e:
        print(f'\n   âœ— Question {i} failed: {str(e)[:100]}')
        results.append({
            'question_id': question['id'],
            'question': query,
            'gold_answer': gold_answer,
            'predicted_answer': '',
            'latency': time.time() - q_start_time,
            'success': False,
            'error': str(e)
        })

    # æ¯50ä¸ªé—®é¢˜ä¿å­˜checkpoint
    if (i + 1) % 50 == 0:
        save_checkpoint(results, 'results/checkpoint_hipporag_hq.json')
        stats = ppr_cache.get_stats()
        print(f'\n   ğŸ’¾ Checkpoint saved at {i+1}/{len(validation)}')
        print(f'   ğŸ“Š PPRç¼“å­˜: å‘½ä¸­ç‡ {stats["hit_rate"]*100:.1f}%, ç¼“å­˜ {stats["cache_size"]}')

    # æ¯100ä¸ªé—®é¢˜æ¸…ç†å†…å­˜
    if (i + 1) % 100 == 0:
        gc.collect()

# æœ€ç»ˆä¿å­˜
save_checkpoint(results, 'results/checkpoint_hipporag_hq.json')

success = sum(1 for r in results if r['success'])
print(f'\nâœ… HippoRAG (é«˜è´¨é‡KG) å®Œæˆ: {success}/{len(results)} ({success/len(results)*100:.1f}%)')

# æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
stats = ppr_cache.get_stats()
print(f'\nğŸ“Š PPRç¼“å­˜ç»Ÿè®¡:')
print(f'   å‘½ä¸­ç‡: {stats["hit_rate"]*100:.1f}%')
print(f'   æ€»å‘½ä¸­: {stats["hits"]}')
print(f'   æ€»æœªå‘½ä¸­: {stats["misses"]}')
print(f'   ç¼“å­˜å¤§å°: {stats["cache_size"]}')

# ä¿å­˜ç»“æœ
Path('results/hipporag_high_quality').mkdir(parents=True, exist_ok=True)
with open('results/hipporag_high_quality/predictions.json', 'w') as f:
    json.dump(results, f, indent=2)

print(f'\nâœ… ç»“æœå·²ä¿å­˜åˆ°: results/hipporag_high_quality/predictions.json')
print()
print('ä¸‹ä¸€æ­¥: å¯¹æ¯”åˆ†æä¸‰ç§æ–¹æ³•çš„æ•ˆæœ')
print('è¿è¡Œ: python scripts/compare_all_results.py')
