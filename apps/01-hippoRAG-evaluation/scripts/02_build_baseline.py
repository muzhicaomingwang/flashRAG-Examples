#!/usr/bin/env python3
"""
Baseline RAG ç´¢å¼•æ„å»ºè„šæœ¬

åŠŸèƒ½ï¼š
1. æ–‡æ¡£åˆ†å—ï¼ˆRecursiveCharacterTextSplitterï¼‰
2. æ„å»º FAISS å‘é‡ç´¢å¼•
3. æ„å»º BM25 ç¨€ç–ç´¢å¼•
4. ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
"""

import os
import sys
import json
import pickle
from pathlib import Path
from typing import List, Dict
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml
import numpy as np
import faiss
import tiktoken
from rank_bm25 import BM25Okapi
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def load_config() -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = project_root / "configs" / "experiment_config.yaml"
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def load_corpus() -> List[Dict]:
    """åŠ è½½æ–‡æ¡£è¯­æ–™åº“ï¼ˆä½¿ç”¨é‡‡æ ·ç‰ˆæœ¬ä»¥èŠ‚çœå†…å­˜ï¼‰"""
    print("ğŸ“š åŠ è½½æ–‡æ¡£è¯­æ–™åº“...")
    # ä¼˜å…ˆä½¿ç”¨é‡‡æ ·ç‰ˆæœ¬ï¼ˆ10Kæ–‡æ¡£ï¼‰ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬
    sampled_path = project_root / "data" / "raw" / "hotpotqa_corpus_sampled.jsonl"
    full_path = project_root / "data" / "raw" / "hotpotqa_corpus.jsonl"

    corpus_path = sampled_path if sampled_path.exists() else full_path

    corpus = []
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line in f:
            corpus.append(json.loads(line))

    print(f"âœ… åŠ è½½å®Œæˆï¼æ€»è®¡ {len(corpus)} ä¸ªæ–‡æ¡£")
    if corpus_path == sampled_path:
        print(f"   ï¼ˆä½¿ç”¨é‡‡æ ·ç‰ˆæœ¬ä»¥èŠ‚çœå†…å­˜ï¼‰")
    return corpus


def chunk_documents(corpus: List[Dict], config: Dict) -> List[Dict]:
    """æ–‡æ¡£åˆ†å—ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨å­—ç¬¦æ•°è¿‘ä¼¼ä»£æ›¿tokenæ•°ï¼‰"""
    print("\nâœ‚ï¸  æ–‡æ¡£åˆ†å—...")

    chunk_config = config['document_processing']
    # ä½¿ç”¨å­—ç¬¦æ•°è¿‘ä¼¼ï¼š1 token â‰ˆ 4 characters
    chunk_size_chars = chunk_config['chunk_size'] * 4
    chunk_overlap_chars = chunk_config['chunk_overlap'] * 4

    chunks = []

    for doc in tqdm(corpus, desc="åˆ†å—è¿›åº¦"):
        doc_text = doc['text']
        text_length = len(doc_text)

        # å¦‚æœæ–‡æ¡£å¾ˆçŸ­ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªå—
        if text_length <= chunk_size_chars:
            chunks.append({
                "chunk_id": f"{doc['id']}_0",
                "doc_id": doc['id'],
                "title": doc['title'],
                "text": doc_text,
                "chunk_index": 0
            })
            continue

        # åˆ†å—ï¼ˆæŒ‰å­—ç¬¦æ•°ï¼‰
        start = 0
        chunk_index = 0

        while start < text_length:
            end = min(start + chunk_size_chars, text_length)
            chunk_text = doc_text[start:end]

            chunks.append({
                "chunk_id": f"{doc['id']}_{chunk_index}",
                "doc_id": doc['id'],
                "title": doc['title'],
                "text": chunk_text,
                "chunk_index": chunk_index
            })

            # ç§»åŠ¨çª—å£
            start = end - chunk_overlap_chars
            chunk_index += 1

    print(f"âœ… åˆ†å—å®Œæˆï¼æ€»è®¡ {len(chunks)} ä¸ªå—")
    print(f"   å¹³å‡æ¯æ–‡æ¡£ {len(chunks) / len(corpus):.1f} ä¸ªå—")

    return chunks


def build_faiss_index(chunks: List[Dict], config: Dict) -> tuple:
    """æ„å»º FAISS å‘é‡ç´¢å¼•"""
    print("\nğŸ”¨ æ„å»º FAISS å‘é‡ç´¢å¼•...")

    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    embedding_config = config['embedding']
    model = embedding_config['model']
    batch_size = embedding_config['batch_size']

    # æå–æ–‡æœ¬
    texts = [chunk['text'] for chunk in chunks]

    # æ‰¹é‡å‘é‡åŒ–
    all_embeddings = []

    for i in tqdm(range(0, len(texts), batch_size), desc="å‘é‡åŒ–è¿›åº¦"):
        batch_texts = texts[i:i + batch_size]

        # è°ƒç”¨ OpenAI Embedding API
        response = client.embeddings.create(
            model=model,
            input=batch_texts
        )

        # æå–å‘é‡
        batch_embeddings = [item.embedding for item in response.data]
        all_embeddings.extend(batch_embeddings)

    # è½¬æ¢ä¸º numpy æ•°ç»„
    embeddings_array = np.array(all_embeddings, dtype='float32')

    # å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    if config['faiss']['normalize_vectors']:
        faiss.normalize_L2(embeddings_array)

    print(f"âœ… å‘é‡åŒ–å®Œæˆï¼ç»´åº¦: {embeddings_array.shape}")

    # æ„å»º FAISS ç´¢å¼•
    print("ğŸ”¨ æ„å»º FAISS IndexFlatL2...")
    dimension = embeddings_array.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings_array)

    print(f"âœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆï¼ç´¢å¼•åŒ…å« {index.ntotal} ä¸ªå‘é‡")

    return index, chunks


def build_bm25_index(chunks: List[Dict], config: Dict) -> BM25Okapi:
    """æ„å»º BM25 ç´¢å¼•"""
    print("\nğŸ”¨ æ„å»º BM25 ç´¢å¼•...")

    # Tokenize æ–‡æ¡£ï¼ˆç®€å•çš„ç©ºæ ¼åˆ†è¯ï¼‰
    tokenized_corpus = [chunk['text'].lower().split() for chunk in chunks]

    # æ„å»º BM25 ç´¢å¼•
    bm25_config = config['bm25']
    bm25 = BM25Okapi(
        tokenized_corpus,
        k1=bm25_config['k1'],
        b=bm25_config['b']
    )

    print(f"âœ… BM25 ç´¢å¼•æ„å»ºå®Œæˆï¼")

    return bm25


def save_indices(faiss_index, bm25_index, chunks: List[Dict], config: Dict) -> None:
    """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
    print("\nğŸ’¾ ä¿å­˜ç´¢å¼•...")

    # ä¿å­˜ FAISS ç´¢å¼•
    faiss_dir = project_root / "data" / "indices" / "faiss"
    faiss_dir.mkdir(parents=True, exist_ok=True)

    faiss_index_path = faiss_dir / "hotpotqa.index"
    faiss.write_index(faiss_index, str(faiss_index_path))
    print(f"âœ… FAISS ç´¢å¼•å·²ä¿å­˜: {faiss_index_path}")

    # ä¿å­˜æ–‡æ¡£æ˜ å°„ï¼ˆid -> chunk å†…å®¹ï¼‰
    doc_mapping_path = faiss_dir / "hotpotqa_docs.pkl"
    with open(doc_mapping_path, 'wb') as f:
        pickle.dump(chunks, f)
    print(f"âœ… æ–‡æ¡£æ˜ å°„å·²ä¿å­˜: {doc_mapping_path}")

    # ä¿å­˜ BM25 ç´¢å¼•
    bm25_dir = project_root / "data" / "indices" / "bm25"
    bm25_dir.mkdir(parents=True, exist_ok=True)

    bm25_path = bm25_dir / "hotpotqa_bm25.pkl"
    with open(bm25_path, 'wb') as f:
        pickle.dump(bm25_index, f)
    print(f"âœ… BM25 ç´¢å¼•å·²ä¿å­˜: {bm25_path}")

    # ä¿å­˜åˆ†å—åçš„æ–‡æ¡£åˆ° processed ç›®å½•
    processed_dir = project_root / "data" / "processed"
    processed_dir.mkdir(parents=True, exist_ok=True)

    chunks_path = processed_dir / "hotpotqa_chunks.jsonl"
    with open(chunks_path, 'w', encoding='utf-8') as f:
        for chunk in chunks:
            f.write(json.dumps(chunk, ensure_ascii=False) + '\n')
    print(f"âœ… åˆ†å—æ–‡æ¡£å·²ä¿å­˜: {chunks_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("Baseline RAG ç´¢å¼•æ„å»º")
    print("=" * 60)

    # åŠ è½½é…ç½®
    config = load_config()

    # åŠ è½½è¯­æ–™åº“
    corpus = load_corpus()

    # æ–‡æ¡£åˆ†å—
    chunks = chunk_documents(corpus, config)

    # æ„å»º FAISS ç´¢å¼•
    faiss_index, chunks_with_embeddings = build_faiss_index(chunks, config)

    # æ„å»º BM25 ç´¢å¼•
    bm25_index = build_bm25_index(chunks, config)

    # ä¿å­˜ç´¢å¼•
    save_indices(faiss_index, bm25_index, chunks, config)

    print("\n" + "=" * 60)
    print("âœ… Baseline RAG æ„å»ºå®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("   python scripts/03_test_baseline.py")


if __name__ == "__main__":
    main()
