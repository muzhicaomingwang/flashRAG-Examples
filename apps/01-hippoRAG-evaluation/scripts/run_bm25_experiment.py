#!/usr/bin/env python3
"""
BM25ç¨€ç–æ£€ç´¢å®éªŒ
å¯¹æ¯”BM25ã€HippoRAGã€FAISSçš„æ€§èƒ½
"""

import json
import pickle
import time
import os
import gc
from pathlib import Path
from rank_bm25 import BM25Okapi
from openai import OpenAI

print('='*80)
print('BM25ç¨€ç–æ£€ç´¢å®éªŒ')
print('='*80)

# åŠ è½½æ•°æ®
print('\nğŸ“š åŠ è½½æ•°æ®...')

validation = []
with open('data/raw/hotpotqa_validation.jsonl', 'r') as f:
    for line in f:
        validation.append(json.loads(line))
print(f'âœ… éªŒè¯é›†: {len(validation)} é—®é¢˜')

# åŠ è½½BM25ç´¢å¼•
with open('data/indices/bm25/hotpotqa_full_bm25.pkl', 'rb') as f:
    bm25_index = pickle.load(f)
print(f'âœ… BM25ç´¢å¼•: {len(bm25_index.doc_len):,} æ–‡æ¡£')

# åŠ è½½chunksï¼ˆä¸FAISSå…±ç”¨ï¼‰
with open('data/indices/faiss/hotpotqa_full_docs.pkl', 'rb') as f:
    chunks = pickle.load(f)
print(f'âœ… Chunks: {len(chunks):,}')

# åˆå§‹åŒ–
client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))

# è¾…åŠ©å‡½æ•°
def load_checkpoint(checkpoint_file):
    if Path(checkpoint_file).exists():
        with open(checkpoint_file, 'r') as f:
            return json.load(f)
    return []

def save_checkpoint(results, checkpoint_file):
    Path(checkpoint_file).parent.mkdir(parents=True, exist_ok=True)
    with open(checkpoint_file, 'w') as f:
        json.dump(results, f, indent=2)

def print_progress(current, total, start_time):
    if current == 0:
        return
    percent = current / total * 100
    elapsed = time.time() - start_time
    rate = elapsed / current
    remaining_min = (rate * (total - current)) / 60
    print(f'   [{current}/{total}] {percent:.1f}% - '
          f'ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ - '
          f'é¢„è®¡å‰©ä½™: {remaining_min:.1f}åˆ†é’Ÿ')

def call_openai_with_retry(func, max_retries=3):
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)

# è¿è¡Œå®éªŒ
print('\n' + '='*80)
print('BM25ç¨€ç–æ£€ç´¢å®éªŒ')
print('='*80)

checkpoint_file = 'results/checkpoint_bm25.json'
results = load_checkpoint(checkpoint_file)
start_idx = len(results)

if start_idx > 0:
    print(f'ğŸ“‹ ä»checkpointæ¢å¤ï¼Œå·²å®Œæˆ: {start_idx}/{len(validation)}')

start_time = time.time()

for i in range(start_idx, len(validation)):
    question = validation[i]

    if i % 10 == 0 or i == start_idx:
        print_progress(i - start_idx, len(validation) - start_idx, start_time)

    query = question['question']
    gold_answer = question['answer']
    q_start_time = time.time()

    try:
        # BM25æ£€ç´¢ï¼ˆåŸºäºå…³é”®è¯ï¼‰
        tokenized_query = query.lower().split()
        scores = bm25_index.get_scores(tokenized_query)

        # è·å–top-5
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:5]
        retrieved_docs = [chunks[idx] for idx in top_indices]

        # æ„å»ºcontext
        context = '\n\n'.join([f"Doc {j+1}: {doc['text']}" for j, doc in enumerate(retrieved_docs)])
        prompt = f"""Answer the question based on the context.

Context:
{context}

Question: {query}

Answer (be concise):"""

        # ç”Ÿæˆç­”æ¡ˆ
        answer_resp = call_openai_with_retry(
            lambda: client.chat.completions.create(
                model='gpt-3.5-turbo',
                messages=[{'role': 'user', 'content': prompt}],
                temperature=0.0,
                max_tokens=256
            )
        )

        predicted = answer_resp.choices[0].message.content.strip()

        results.append({
            'question_id': question['id'],
            'question': query,
            'gold_answer': gold_answer,
            'predicted_answer': predicted,
            'latency': time.time() - q_start_time,
            'success': True
        })

    except Exception as e:
        print(f'\n   âœ— Question {i} failed: {str(e)[:100]}')
        results.append({
            'question_id': question['id'],
            'question': query,
            'gold_answer': gold_answer,
            'predicted_answer': '',
            'latency': time.time() - q_start_time,
            'success': False,
            'error': str(e)
        })

    # æ¯50ä¸ªé—®é¢˜ä¿å­˜checkpoint
    if (i + 1) % 50 == 0:
        save_checkpoint(results, checkpoint_file)
        print(f'\n   ğŸ’¾ Checkpoint saved at {i+1}/{len(validation)}')

    # æ¯100ä¸ªé—®é¢˜æ¸…ç†å†…å­˜
    if (i + 1) % 100 == 0:
        gc.collect()

# æœ€ç»ˆä¿å­˜
save_checkpoint(results, checkpoint_file)

success = sum(1 for r in results if r['success'])
print(f'\nâœ… BM25å®éªŒå®Œæˆ: {success}/{len(results)} ({success/len(results)*100:.1f}%)')

# ä¿å­˜æœ€ç»ˆç»“æœ
Path('results/bm25').mkdir(parents=True, exist_ok=True)
with open('results/bm25/predictions.json', 'w') as f:
    json.dump(results, f, indent=2)

print(f'\nâœ… ç»“æœå·²ä¿å­˜åˆ°: results/bm25/predictions.json')
print()
print('ä¸‹ä¸€æ­¥: ç”Ÿæˆå››æ–¹å¯¹æ¯”æŠ¥å‘Š')
print('è¿è¡Œ: python scripts/compare_four_methods.py')
