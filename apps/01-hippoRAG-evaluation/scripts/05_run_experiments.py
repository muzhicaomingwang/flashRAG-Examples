#!/usr/bin/env python3
"""
å®Œæ•´å®éªŒè¿è¡Œè„šæœ¬

åŠŸèƒ½ï¼š
1. è¿è¡Œ Baseline RAG
2. è¿è¡Œ HippoRAG-Simple
3. è¿è¡Œ HippoRAG-Full
4. è¯„ä¼°å¹¶ä¿å­˜ç»“æœ
"""

import os
import sys
import json
import pickle
import time
from pathlib import Path
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml
import numpy as np
import faiss
import spacy
import networkx as nx
from tqdm import tqdm
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def load_config() -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = project_root / "configs" / "experiment_config.yaml"
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def load_validation_set() -> List[Dict]:
    """åŠ è½½éªŒè¯é›†"""
    val_path = project_root / "data" / "raw" / "hotpotqa_validation.jsonl"

    validation = []
    with open(val_path, 'r', encoding='utf-8') as f:
        for line in f:
            validation.append(json.loads(line))

    return validation


def load_baseline_indices():
    """åŠ è½½ Baseline RAG ç´¢å¼•"""
    # åŠ è½½ FAISS ç´¢å¼•
    faiss_index_path = project_root / "data" / "indices" / "faiss" / "hotpotqa.index"
    faiss_index = faiss.read_index(str(faiss_index_path))

    # åŠ è½½æ–‡æ¡£æ˜ å°„
    doc_mapping_path = project_root / "data" / "indices" / "faiss" / "hotpotqa_docs.pkl"
    with open(doc_mapping_path, 'rb') as f:
        chunks = pickle.load(f)

    return faiss_index, chunks


def load_hipporag_kg():
    """åŠ è½½ HippoRAG çŸ¥è¯†å›¾è°±"""
    kg_path = project_root / "data" / "knowledge_graphs" / "hotpotqa_kg.gpickle"
    kg = nx.read_gpickle(str(kg_path))

    pr_path = project_root / "data" / "knowledge_graphs" / "hotpotqa_pagerank.pkl"
    with open(pr_path, 'rb') as f:
        pagerank_scores = pickle.load(f)

    return kg, pagerank_scores


class BaselineRAG:
    """Baseline RAG ç³»ç»Ÿ"""

    def __init__(self, faiss_index, chunks, client, config):
        self.faiss_index = faiss_index
        self.chunks = chunks
        self.client = client
        self.config = config

    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # å‘é‡åŒ–æŸ¥è¯¢
        response = self.client.embeddings.create(
            model=self.config['embedding']['model'],
            input=[query]
        )
        query_vector = np.array([response.data[0].embedding], dtype='float32')

        # å½’ä¸€åŒ–
        if self.config['faiss']['normalize_vectors']:
            faiss.normalize_L2(query_vector)

        # æ£€ç´¢
        distances, indices = self.faiss_index.search(query_vector, top_k)

        # è·å–æ–‡æ¡£
        results = [self.chunks[idx] for idx in indices[0]]
        return results

    def answer(self, query: str, retrieved_docs: List[Dict]) -> str:
        """åŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»º prompt
        context = "\n\n".join([
            f"Document {i+1} ({doc['title']}):\n{doc['text']}"
            for i, doc in enumerate(retrieved_docs)
        ])

        prompt = f"""Answer the question based on the provided context.

Context:
{context}

Question: {query}

Answer (be concise):"""

        # è°ƒç”¨ LLM
        response = self.client.chat.completions.create(
            model=self.config['baseline_rag']['llm_model'],
            messages=[{"role": "user", "content": prompt}],
            temperature=self.config['baseline_rag']['llm_temperature'],
            max_tokens=self.config['baseline_rag']['llm_max_tokens']
        )

        return response.choices[0].message.content.strip()


class HippoRAG:
    """HippoRAG ç³»ç»Ÿ"""

    def __init__(self, faiss_index, chunks, kg, pagerank_scores, client, config, nlp):
        self.faiss_index = faiss_index
        self.chunks = chunks
        self.kg = kg
        self.pagerank_scores = pagerank_scores
        self.client = client
        self.config = config
        self.nlp = nlp

        # æ„å»º chunk_id -> index çš„æ˜ å°„
        self.chunk_id_to_idx = {chunk['chunk_id']: i for i, chunk in enumerate(chunks)}

    def retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """ä½¿ç”¨ KG + PPR æ£€ç´¢"""
        # Step 1: åˆæ£€ç´¢ï¼ˆè·å–å€™é€‰é›†ï¼‰
        initial_k = self.config['hipporag']['retrieval']['initial_k']

        response = self.client.embeddings.create(
            model=self.config['embedding']['model'],
            input=[query]
        )
        query_vector = np.array([response.data[0].embedding], dtype='float32')

        if self.config['faiss']['normalize_vectors']:
            faiss.normalize_L2(query_vector)

        distances, indices = self.faiss_index.search(query_vector, initial_k)

        # Step 2: ä»æŸ¥è¯¢ä¸­æå–å®ä½“
        query_entities = extract_entities_spacy(query, self.nlp)
        query_entity_ids = [f"entity_{name.replace(' ', '_')}" for name, _ in query_entities]

        # è¿‡æ»¤ï¼šåªä¿ç•™å›¾ä¸­å­˜åœ¨çš„å®ä½“
        query_entity_ids = [eid for eid in query_entity_ids if self.kg.has_node(eid)]

        # Step 3: ä¸ªæ€§åŒ– PageRank
        if query_entity_ids:
            # æ„å»ºä¸ªæ€§åŒ–å‘é‡ï¼ˆæŸ¥è¯¢å®ä½“çš„æƒé‡æ›´é«˜ï¼‰
            personalization = {node: 0.0 for node in self.kg.nodes}
            for eid in query_entity_ids:
                personalization[eid] = 1.0 / len(query_entity_ids)

            # è®¡ç®—ä¸ªæ€§åŒ– PageRank
            ppr_scores = nx.pagerank(
                self.kg,
                alpha=self.config['hipporag']['pagerank']['damping_factor'],
                personalization=personalization,
                max_iter=self.config['hipporag']['pagerank']['max_iterations']
            )
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®ä½“ï¼Œä½¿ç”¨å…¨å±€ PageRank
            ppr_scores = self.pagerank_scores

        # Step 4: å¯¹å€™é€‰æ–‡æ¡£å—é‡æ–°æ’åº
        candidate_chunks = [self.chunks[idx] for idx in indices[0]]

        # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆPPR + åˆæ£€ç´¢è·ç¦»ï¼‰
        reranked = []
        for chunk, distance in zip(candidate_chunks, distances[0]):
            chunk_id = chunk['chunk_id']

            # PPR åˆ†æ•°ï¼ˆå¦‚æœchunkåœ¨å›¾ä¸­ï¼‰
            ppr_score = ppr_scores.get(chunk_id, 0.0)

            # åˆæ£€ç´¢åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
            retrieval_score = 1.0 / (1.0 + float(distance))

            # ç»¼åˆå¾—åˆ†
            combined_score = 0.5 * ppr_score + 0.5 * retrieval_score

            reranked.append({
                "chunk": chunk,
                "ppr_score": ppr_score,
                "retrieval_score": retrieval_score,
                "combined_score": combined_score
            })

        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        reranked.sort(key=lambda x: x['combined_score'], reverse=True)

        # è¿”å› top-k
        return [item['chunk'] for item in reranked[:top_k]]

    def answer(self, query: str, retrieved_docs: List[Dict]) -> str:
        """åŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆï¼ˆä¸ Baseline ç›¸åŒï¼‰"""
        context = "\n\n".join([
            f"Document {i+1} ({doc['title']}):\n{doc['text']}"
            for i, doc in enumerate(retrieved_docs)
        ])

        prompt = f"""Answer the question based on the provided context.

Context:
{context}

Question: {query}

Answer (be concise):"""

        response = self.client.chat.completions.create(
            model=self.config['hipporag']['retrieval']['llm_model'],
            messages=[{"role": "user", "content": prompt}],
            temperature=self.config['hipporag']['retrieval']['llm_temperature'],
            max_tokens=self.config['hipporag']['retrieval']['llm_max_tokens']
        )

        return response.choices[0].message.content.strip()


def run_single_experiment(method_name: str, retriever, question: Dict) -> Dict:
    """è¿è¡Œå•ä¸ªé—®é¢˜çš„å®éªŒ"""
    query = question['question']
    gold_answer = question['answer']

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    try:
        # æ£€ç´¢
        retrieved_docs = retriever.retrieve(query)

        # ç”Ÿæˆç­”æ¡ˆ
        predicted_answer = retriever.answer(query, retrieved_docs)

        # è®°å½•æ—¶é—´
        latency = time.time() - start_time

        return {
            "question_id": question['id'],
            "question": query,
            "gold_answer": gold_answer,
            "predicted_answer": predicted_answer,
            "retrieved_docs": [doc['chunk_id'] for doc in retrieved_docs],
            "latency": latency,
            "success": True,
            "error": None
        }

    except Exception as e:
        return {
            "question_id": question['id'],
            "question": query,
            "gold_answer": gold_answer,
            "predicted_answer": "",
            "retrieved_docs": [],
            "latency": time.time() - start_time,
            "success": False,
            "error": str(e)
        }


def run_experiment(method_name: str, retriever, validation_set: List[Dict], config: Dict) -> List[Dict]:
    """è¿è¡Œå®Œæ•´å®éªŒ"""
    print(f"\n{'='*60}")
    print(f"è¿è¡Œå®éªŒ: {method_name}")
    print(f"{'='*60}")

    results = []

    # å¹¶è¡Œè¿è¡Œï¼ˆåŠ é€Ÿï¼‰
    max_workers = config['api']['max_concurrent_requests']

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for question in validation_set:
            future = executor.submit(run_single_experiment, method_name, retriever, question)
            futures.append(future)

        # ä½¿ç”¨è¿›åº¦æ¡
        for future in tqdm(as_completed(futures), total=len(futures), desc=f"{method_name} è¿›åº¦"):
            result = future.result()
            results.append(result)

    # ç»Ÿè®¡æˆåŠŸç‡
    success_count = sum(1 for r in results if r['success'])
    print(f"\nâœ… å®éªŒå®Œæˆï¼æˆåŠŸç‡: {success_count}/{len(results)}")

    return results


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("HippoRAG å¯¹æ¯”å®éªŒ")
    print("=" * 80)

    # åŠ è½½é…ç½®
    config = load_config()

    # åŠ è½½éªŒè¯é›†
    print("\nğŸ“š åŠ è½½æ•°æ®...")
    validation_set = load_validation_set()
    print(f"âœ… éªŒè¯é›†: {len(validation_set)} ä¸ªé—®é¢˜")

    # åŠ è½½ç´¢å¼•
    print("\nğŸ“š åŠ è½½ç´¢å¼•...")
    faiss_index, chunks = load_baseline_indices()
    print(f"âœ… FAISS ç´¢å¼•: {faiss_index.ntotal:,} ä¸ªå‘é‡")

    kg, pagerank_scores = load_hipporag_kg()
    print(f"âœ… çŸ¥è¯†å›¾è°±: {kg.number_of_nodes():,} ä¸ªèŠ‚ç‚¹")

    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    # åŠ è½½ SpaCy
    nlp = spacy.load("en_core_web_sm")

    # å®éªŒ 1: Baseline RAG
    print("\n" + "="*80)
    print("å®éªŒ 1/3: Baseline RAG")
    print("="*80)

    baseline_rag = BaselineRAG(faiss_index, chunks, client, config)
    baseline_results = run_experiment("Baseline-RAG", baseline_rag, validation_set, config)

    # ä¿å­˜ä¸­é—´ç»“æœ
    results_dir = project_root / "results" / "baseline"
    results_dir.mkdir(parents=True, exist_ok=True)
    with open(results_dir / "predictions.json", 'w') as f:
        json.dump(baseline_results, indent=2, fp=f)

    # å®éªŒ 2 & 3: HippoRAG
    print("\n" + "="*80)
    print("å®éªŒ 2/3: HippoRAG")
    print("="*80)

    hipporag = HippoRAG(faiss_index, chunks, kg, pagerank_scores, client, config, nlp)
    hipporag_results = run_experiment("HippoRAG", hipporag, validation_set, config)

    # ä¿å­˜ä¸­é—´ç»“æœ
    results_dir = project_root / "results" / "hipporag"
    results_dir.mkdir(parents=True, exist_ok=True)
    with open(results_dir / "predictions.json", 'w') as f:
        json.dump(hipporag_results, indent=2, fp=f)

    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰å®éªŒå®Œæˆï¼")
    print("="*80)
    print("\nğŸ“ ä¸‹ä¸€æ­¥:")
    print("   python scripts/06_generate_report.py")


if __name__ == "__main__":
    main()
