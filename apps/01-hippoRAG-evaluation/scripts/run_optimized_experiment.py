#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆå¯¹æ¯”å®éªŒ
ä½¿ç”¨å®Œæ•´66Kæ–‡æ¡£ç´¢å¼•å’Œ70%è¦†ç›–çš„KG
"""

import json
import pickle
import time
import os
from pathlib import Path
import numpy as np
import faiss
import spacy
import networkx as nx
from openai import OpenAI

print('='*80)
print('ä¼˜åŒ–ç‰ˆHippoRAGå¯¹æ¯”å®éªŒï¼ˆ66Kæ–‡æ¡£ï¼‰')
print('='*80)

# åŠ è½½
print('\nğŸ“š åŠ è½½æ•°æ®å’Œç´¢å¼•...')

validation = []
with open('data/raw/hotpotqa_validation.jsonl', 'r') as f:
    for line in f:
        validation.append(json.loads(line))
print(f'âœ… éªŒè¯é›†: {len(validation)} é—®é¢˜')

# ä½¿ç”¨_fullç‰ˆæœ¬çš„ç´¢å¼•
faiss_index = faiss.read_index('data/indices/faiss/hotpotqa_full.index')
with open('data/indices/faiss/hotpotqa_full_docs.pkl', 'rb') as f:
    chunks = pickle.load(f)
print(f'âœ… FAISS: {faiss_index.ntotal:,} å‘é‡')

# ä½¿ç”¨_fullç‰ˆæœ¬çš„KG
with open('data/knowledge_graphs/hotpotqa_kg_full.gpickle', 'rb') as f:
    kg = pickle.load(f)
with open('data/knowledge_graphs/hotpotqa_pagerank_full.pkl', 'rb') as f:
    pagerank_scores = pickle.load(f)
print(f'âœ… çŸ¥è¯†å›¾è°±: {kg.number_of_nodes():,} èŠ‚ç‚¹, {kg.number_of_edges():,} è¾¹')

client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))
nlp = spacy.load('en_core_web_sm')

chunk_id_to_idx = {chunk['chunk_id']: i for i, chunk in enumerate(chunks)}

# å®éªŒ1: Baseline
print('\n' + '='*80)
print('å®éªŒ 1/2: Baseline RAGï¼ˆ66Kæ–‡æ¡£ï¼‰')
print('='*80)

baseline_results = []

for i, question in enumerate(validation):
    if i % 50 == 0:
        print(f'   è¿›åº¦: {i}/{len(validation)}')
    
    query = question['question']
    gold_answer = question['answer']
    start_time = time.time()
    
    try:
        resp = client.embeddings.create(model='text-embedding-3-small', input=[query])
        q_vec = np.array([resp.data[0].embedding], dtype='float32')
        faiss.normalize_L2(q_vec)
        
        distances, indices = faiss_index.search(q_vec, 5)
        retrieved_docs = [chunks[idx] for idx in indices[0]]
        
        context = '\n\n'.join([f"Doc {j+1}: {doc['text']}" for j, doc in enumerate(retrieved_docs)])
        prompt = f"""Answer the question based on the context.

Context:
{context}

Question: {query}

Answer (be concise):"""
        
        answer_resp = client.chat.completions.create(
            model='gpt-3.5-turbo',
            messages=[{'role': 'user', 'content': prompt}],
            temperature=0.0,
            max_tokens=256
        )
        
        predicted = answer_resp.choices[0].message.content.strip()
        
        baseline_results.append({
            'question_id': question['id'],
            'question': query,
            'gold_answer': gold_answer,
            'predicted_answer': predicted,
            'latency': time.time() - start_time,
            'success': True
        })
        
    except Exception as e:
        baseline_results.append({
            'question_id': question['id'],
            'question': query,
            'gold_answer': gold_answer,
            'predicted_answer': '',
            'latency': time.time() - start_time,
            'success': False,
            'error': str(e)
        })

success = sum(1 for r in baseline_results if r['success'])
print(f'\nâœ… Baselineå®Œæˆ: {success}/{len(baseline_results)}')

Path('results/baseline_full').mkdir(parents=True, exist_ok=True)
with open('results/baseline_full/predictions.json', 'w') as f:
    json.dump(baseline_results, indent=2, fp=f)

# å®éªŒ2: HippoRAG
print('\n' + '='*80)
print('å®éªŒ 2/2: HippoRAGï¼ˆ66Kæ–‡æ¡£ï¼Œ70% KGï¼‰')
print('='*80)

hipporag_results = []

for i, question in enumerate(validation):
    if i % 50 == 0:
        print(f'   è¿›åº¦: {i}/{len(validation)}')
    
    query = question['question']
    gold_answer = question['answer']
    start_time = time.time()
    
    try:
        resp = client.embeddings.create(model='text-embedding-3-small', input=[query])
        q_vec = np.array([resp.data[0].embedding], dtype='float32')
        faiss.normalize_L2(q_vec)
        
        distances, indices = faiss_index.search(q_vec, 20)
        
        query_doc = nlp(query)
        query_entities = [ent.text.lower().strip() for ent in query_doc.ents]
        query_entity_ids = [f"entity_{e.replace(' ', '_')}" for e in query_entities]
        query_entity_ids = [eid for eid in query_entity_ids if kg.has_node(eid)]
        
        if query_entity_ids:
            personalization = {node: 0.0 for node in kg.nodes}
            for eid in query_entity_ids:
                personalization[eid] = 1.0 / len(query_entity_ids)
            ppr_scores = nx.pagerank(kg, alpha=0.85, personalization=personalization, max_iter=100)
        else:
            ppr_scores = pagerank_scores
        
        candidate_chunks = [chunks[idx] for idx in indices[0]]
        reranked = []
        
        for chunk, distance in zip(candidate_chunks, distances[0]):
            chunk_id = chunk['chunk_id']
            ppr_score = ppr_scores.get(chunk_id, 0.0)
            retrieval_score = 1.0 / (1.0 + float(distance))
            combined_score = 0.5 * ppr_score + 0.5 * retrieval_score
            reranked.append({'chunk': chunk, 'combined_score': combined_score})
        
        reranked.sort(key=lambda x: x['combined_score'], reverse=True)
        retrieved_docs = [item['chunk'] for item in reranked[:5]]
        
        context = '\n\n'.join([f"Doc {j+1}: {doc['text']}" for j, doc in enumerate(retrieved_docs)])
        prompt = f"""Answer the question based on the context.

Context:
{context}

Question: {query}

Answer (be concise):"""
        
        answer_resp = client.chat.completions.create(
            model='gpt-3.5-turbo',
            messages=[{'role': 'user', 'content': prompt}],
            temperature=0.0,
            max_tokens=256
        )
        
        predicted = answer_resp.choices[0].message.content.strip()
        
        hipporag_results.append({
            'question_id': question['id'],
            'question': query,
            'gold_answer': gold_answer,
            'predicted_answer': predicted,
            'latency': time.time() - start_time,
            'success': True
        })
        
    except Exception as e:
        hipporag_results.append({
            'question_id': question['id'],
            'question': query,
            'gold_answer': gold_answer,
            'predicted_answer': '',
            'latency': time.time() - start_time,
            'success': False,
            'error': str(e)
        })

success = sum(1 for r in hipporag_results if r['success'])
print(f'\nâœ… HippoRAGå®Œæˆ: {success}/{len(hipporag_results)}')

Path('results/hipporag_full').mkdir(parents=True, exist_ok=True)
with open('results/hipporag_full/predictions.json', 'w') as f:
    json.dump(hipporag_results, indent=2, fp=f)

print('\n' + '='*80)
print('âœ… ä¼˜åŒ–ç‰ˆå®éªŒå®Œæˆï¼')
print('='*80)
